{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0. Exploratory analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_training=pd.read_json('project_files/training.json', encoding = 'utf8')\n",
    "df_devel=pd.read_json('project_files/devel.json')\n",
    "df_docs=pd.read_json('project_files/documents.json')\n",
    "df_testing=pd.read_json('project_files/testing.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_training['text'][:1]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_paragraph</th>\n",
       "      <th>docid</th>\n",
       "      <th>question</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>A kilogram could be definined as having a Plan...</td>\n",
       "      <td>6966662606895999999♠6.62606896×10−34 j⋅s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the shape of the object that establish...</td>\n",
       "      <td>cylinder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>What example is given as another paired relati...</td>\n",
       "      <td>time vs. energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>What does the Planck Constant refer to?</td>\n",
       "      <td>quantum of action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>When was the first quantized model of the atom...</td>\n",
       "      <td>1913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_paragraph  docid                                           question  \\\n",
       "0                23      0  A kilogram could be definined as having a Plan...   \n",
       "1                22      0  What is the shape of the object that establish...   \n",
       "2                12      0  What example is given as another paired relati...   \n",
       "3                 1      0            What does the Planck Constant refer to?   \n",
       "4                10      0  When was the first quantized model of the atom...   \n",
       "\n",
       "                                       text  \n",
       "0  6966662606895999999♠6.62606896×10−34 j⋅s  \n",
       "1                                  cylinder  \n",
       "2                           time vs. energy  \n",
       "3                         quantum of action  \n",
       "4                                      1913  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[First recognized in 1900 by Max Planck, it wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Public policy and political leadership helps ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[In a career spanning more than four decades, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Anti-aircraft warfare or counter-air defence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[The Endangered Species Act of 1973 (ESA; 16 U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   docid                                               text\n",
       "0      0  [First recognized in 1900 by Max Planck, it wa...\n",
       "1      1  [Public policy and political leadership helps ...\n",
       "2      2  [In a career spanning more than four decades, ...\n",
       "3      3  [Anti-aircraft warfare or counter-air defence ...\n",
       "4      4  [The Endangered Species Act of 1973 (ESA; 16 U..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_devel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Question processing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "def process_question(words):\n",
    "    ## question type: classify the type of question\n",
    "    # hand writter rule vs supervised ML - start with rules then we can move to supervised using datasets like.\n",
    "    \n",
    "    # default values\n",
    "    question_type='UNK'\n",
    "    answer_type='UNK'\n",
    "    query=''\n",
    "    \n",
    "    # handwriten rules\n",
    "    y_n_question=['is','can','could']\n",
    "    wh_question=['what','who','where','when']\n",
    "    \n",
    "    for word in words:\n",
    "        word=word.lower()\n",
    "        if word in y_n_question:\n",
    "            question_type='YN'\n",
    "            break\n",
    "        elif word in wh_question:\n",
    "            question_type='WH'\n",
    "            break\n",
    "        else:\n",
    "            question_type='UNK'\n",
    "            \n",
    "    \n",
    "    ## focus: strings possible to be replaced in the answer\n",
    "    # identify with the NER - use NLP taggin from CoreNLP\n",
    "    \n",
    "    ## answer type: kind of entity\n",
    "    # from NER\n",
    "    \n",
    "    # just to start, the answer type is the same as question -- DELETE\n",
    "    answer_type=question_type\n",
    "    \n",
    "    ## query: keywords to be used for the IR system to search in documents\n",
    "    # do we need to do semantic parsing?\n",
    "    # query should be built according to question type\n",
    "    \n",
    "    \n",
    "    \n",
    "    return query,answer_type\n",
    "\n",
    "# process questions\n",
    "queries=[]\n",
    "for index, row in df_training.iterrows():\n",
    "    question_training=nltk.word_tokenize(row['question'])\n",
    "    query,answer_type=process_question(question_training)\n",
    "    queries.append((query,answer_type))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>440</td>\n",
       "      <td>3613</td>\n",
       "      <td>Which number of ranks wore purple clothing in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3614</th>\n",
       "      <td>440</td>\n",
       "      <td>3614</td>\n",
       "      <td>In what year did nations standardize on red as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3615</th>\n",
       "      <td>440</td>\n",
       "      <td>3615</td>\n",
       "      <td>In what year was the Community Party of China ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3616</th>\n",
       "      <td>440</td>\n",
       "      <td>3616</td>\n",
       "      <td>What was Turkey red called in France?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3617</th>\n",
       "      <td>440</td>\n",
       "      <td>3617</td>\n",
       "      <td>What other pigment was Turkey red compared to?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      docid    id                                           question\n",
       "3613    440  3613  Which number of ranks wore purple clothing in ...\n",
       "3614    440  3614  In what year did nations standardize on red as...\n",
       "3615    440  3615  In what year was the Community Party of China ...\n",
       "3616    440  3616              What was Turkey red called in France?\n",
       "3617    440  3617     What other pigment was Turkey red compared to?"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence.strip()) for sentence in df_training['question']]\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'A', u'kilogram', u'could', u'be', u'definined', u'as', u'having', u'a', u'Planck', u'constant', u'of', u'what', u'value', u'?']\n"
     ]
    }
   ],
   "source": [
    "print tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'A', 'DT'), (u'kilogram', 'NN'), (u'could', 'MD'), (u'be', 'VB'), (u'definined', 'VBN'), (u'as', 'IN'), (u'having', 'VBG'), (u'a', 'DT'), (u'Planck', 'NNP'), (u'constant', 'NN'), (u'of', 'IN'), (u'what', 'WP'), (u'value', 'NN'), (u'?', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "print tagged_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "entity_names = []\n",
    "entity_name = []\n",
    "for tree in chunked_sentences:\n",
    "    # Print results per sentence\n",
    "    print tree\n",
    "    print extract_entity_names(tree)\n",
    "    entity_names.extend(extract_entity_names(tree))\n",
    "    entity_name.append(extract_entity_names(tree))\n",
    "\n",
    "# Print all entity names\n",
    "#print entity_names\n",
    "\n",
    "# Print unique entity names\n",
    "# print set(entity_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find Keywords\n",
    "2. Answer types - Using answer type taxonomy\n",
    "3. Query formulation -> Keywords\n",
    "4. Go to each document and check the frequency distribution of words and pick the document if one of the query words are present in document. Create a rank with that score\n",
    "5. Find the paragraphs -> Discard irrelevant paragraphs. Use NE,Keywords, longest exact keywords. Put same weight for now and calculate the score of paragraphs. Rank each of the paragraphs in the document. We have to use the original answer and match the answer type\n",
    "6. Find candidate answers -> Use supervised ML method\n",
    "7. Merge candidate answers -> Use NER\n",
    "8. Pick the best answer -> Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Candidate answering generation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Get a score for the passage to filter the most relevant passages</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## features relevant to this part\n",
    "# number of named entities of the right type in the passage\n",
    "# number of question keywords in the passage\n",
    "# the longest exact sequence of question keywords\n",
    "# rank of the document where the passage was extracted\n",
    "# proximity of the keywords from the original query\n",
    "# ngram overlap between the passage and the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will set up useful functions to extract term frequencies to build the vector space model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "# get the terms for a passage\n",
    "def get_terms(passage):\n",
    "    terms = set()\n",
    "    for token in nltk.word_tokenize(passage):\n",
    "        if token not in stopwords: \n",
    "            terms.add(stemmer.stem(token.lower()))\n",
    "    return terms\n",
    "    \n",
    "# get document_term \n",
    "def get_document_term_passsages(ds_documents):\n",
    "    document_term={}\n",
    "    passageID=0\n",
    "    for index, row in ds_documents.iterrows():\n",
    "        # every row is a document\n",
    "        list_of_passages=row['text']\n",
    "        for passage in list_of_passages:\n",
    "            terms=get_terms(passage)\n",
    "            document_term[passageID]=terms\n",
    "            passageID+=1\n",
    "        \n",
    "    return document_term\n",
    "\n",
    "# get the term frequency\n",
    "def extract_term_freqs(doc):\n",
    "    tfs = Counter()\n",
    "    for token in doc:\n",
    "        if token not in stopwords: \n",
    "            tfs[stemmer.stem(token.lower())] += 1\n",
    "    return tfs\n",
    "        \n",
    "# compute idf\n",
    "def compute_doc_freqs(doc_term_freqs):\n",
    "    dfs = Counter()\n",
    "    for tfs in doc_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            dfs[term] += 1\n",
    "    return dfs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document-term matrix\n",
    "docs=get_document_term_passsages(df_docs.iloc[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector space model we need to define a score function\n",
    "# first I will use tf-idf\n",
    "doc_term_freqs = {}\n",
    "for docid, terms in docs.items():\n",
    "    term_freqs = extract_term_freqs(terms)\n",
    "    doc_term_freqs[docid] = term_freqs\n",
    "\n",
    "M = len(doc_term_freqs)\n",
    "\n",
    "doc_freqs = compute_doc_freqs(doc_term_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Improvement:</b> Use BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inverted index for query processing. Inverted index will not change from query to query. Here we can improve how the weight is defined for the posting list tuple for each term (docid,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code from WSTA_N16_information_retrieval\n",
    "vsm_inverted_index = defaultdict(list)\n",
    "for docid, term_freqs in doc_term_freqs.items():\n",
    "    N = sum(term_freqs.values())\n",
    "    length = 0\n",
    "    \n",
    "    # find tf*idf values and accumulate sum of squares \n",
    "    tfidf_values = []\n",
    "    for term, count in term_freqs.items():\n",
    "        tfidf = float(count) / N * log(M / float(doc_freqs[term]))\n",
    "        tfidf_values.append((term, tfidf))\n",
    "        length += tfidf ** 2\n",
    "\n",
    "    # normalise documents by length and insert into index\n",
    "    length = length ** 0.5\n",
    "    for term, tfidf in tfidf_values:\n",
    "        # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "        vsm_inverted_index[term].append([docid, tfidf / length])\n",
    "        \n",
    "# ensure posting lists are in sorted order (less important here cf above)\n",
    "for term, docids in vsm_inverted_index.items():\n",
    "    docids.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the VSM creating a score for each document (passage) and returning the top k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vsm(query, index, k=10):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator.most_common(k)\n",
    "\n",
    "## end copied code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.680116589444509),\n",
       " (1, 0.30340274351915997),\n",
       " (6, 0.24364584220578492),\n",
       " (65, 0.17637606987370624),\n",
       " (105, 0.16152465265290256),\n",
       " (10, 0.14946948590599873),\n",
       " (3, 0.14114017042707586),\n",
       " (14, 0.13829184288746416),\n",
       " (11, 0.13227012047661246),\n",
       " (19, 0.10701743930986322)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = query_vsm([stemmer.stem(term.lower()) for term in \"First recognized in 1900 by Max Planck\".split()], vsm_inverted_index)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Candidate answering scoring</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Answer and confidence</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
