{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0. Exploratory analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_training=pd.read_json('project_files/training.json')\n",
    "df_devel=pd.read_json('project_files/devel.json')\n",
    "df_docs=pd.read_json('project_files/documents.json')\n",
    "df_testing=pd.read_json('project_files/testing.json')\n",
    "\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_training['text'][:1]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_devel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Question processing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "def process_question(words):\n",
    "    ## question type: classify the type of question\n",
    "    # hand writter rule vs supervised ML - start with rules then we can move to supervised using datasets like.\n",
    "    \n",
    "    # default values\n",
    "    question_type='UNK'\n",
    "    answer_type='UNK'\n",
    "    query=''\n",
    "    \n",
    "    # handwriten rules\n",
    "    y_n_question=['is','can','could']\n",
    "    wh_question=['what','who','where','when']\n",
    "    \n",
    "    for word in words:\n",
    "        word=word.lower()\n",
    "        if word in y_n_question:\n",
    "            question_type='YN'\n",
    "            break\n",
    "        elif word in wh_question:\n",
    "            question_type='WH'\n",
    "            break\n",
    "        else:\n",
    "            question_type='UNK'\n",
    "            \n",
    "    \n",
    "    ## focus: strings possible to be replaced in the answer\n",
    "    # identify with the NER - use NLP taggin from CoreNLP\n",
    "    \n",
    "    ## answer type: kind of entity\n",
    "    # from NER\n",
    "    \n",
    "    # just to start, the answer type is the same as question -- DELETE\n",
    "    answer_type=question_type\n",
    "    \n",
    "    ## query: keywords to be used for the IR system to search in documents\n",
    "    # do we need to do semantic parsing?\n",
    "    # query should be built according to question type\n",
    "    \n",
    "    \n",
    "    \n",
    "    return query,answer_type\n",
    "\n",
    "# process questions\n",
    "queries=[]\n",
    "for index, row in df_training.iterrows():\n",
    "    question_training=nltk.word_tokenize(row['question'])\n",
    "    query,answer_type=process_question(question_training)\n",
    "    queries.append((query,answer_type))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find Keywords\n",
    "2. Answer types - Using answer type taxonomy\n",
    "3. Query formulation -> Keywords\n",
    "4. Go to each document and check the frequency distribution of words and pick the document if one of the query words are present in document. Create a rank with that score\n",
    "5. Find the paragraphs -> Discard irrelevant paragraphs. Use NE,Keywords, longest exact keywords. Put same weight for now and calculate the score of paragraphs. Rank each of the paragraphs in the document. We have to use the original answer and match the answer type\n",
    "6. Find candidate answers -> Use supervised ML method\n",
    "7. Merge candidate answers -> Use NER\n",
    "8. Pick the best answer -> Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NER using NLTK OR Stanford CORENLP</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring Stanford CoreNLP . Link -> https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "def get_Name_Entity_NLTK(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        sentence = sentence.encode('utf8')\n",
    "        ne_chunked_sents = ne_chunk(pos_tag(word_tokenize(unicode(sentence, errors='ignore'))))\n",
    "        result = []\n",
    "#         print (sentence)\n",
    "        for tagged_tree in ne_chunked_sents:\n",
    "#             print (tagged_tree)\n",
    "            if hasattr(tagged_tree, 'label'):\n",
    "                entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "                entity_type = tagged_tree.label() # get NE category\n",
    "                result.append((entity_name.decode('utf8'), entity_type))\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_Name_Entity_Sentence(sentence):\n",
    "    st = StanfordNERTagger('english.muc.7class.distsim.crf.ser.gz')\n",
    "    sentence = sentence.encode('utf8')\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    result = {}\n",
    "    \n",
    "    for res in classified_text:\n",
    "        if len(res) > 0:\n",
    "            entity_name = res[0]\n",
    "            entity_type = res[1]\n",
    "            if len(entity_name) > 1 and len(entity_type) > 1:\n",
    "                result[entity_name.strip()] = entity_type.strip()\n",
    "    \n",
    "    ne_chunked_sents = ne_chunk(pos_tag(word_tokenize(unicode(sentence, errors='ignore'))))\n",
    "    for tagged_tree in ne_chunked_sents:\n",
    "        if hasattr(tagged_tree, 'label'):\n",
    "            entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "            entity_type = tagged_tree.label() # get NE category\n",
    "            result[entity_name.decode('utf8')] = entity_type\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_Name_Entity_paragraph(paragraph):\n",
    "    result = []\n",
    "    tokenized_sentence = nltk.sent_tokenize(paragraph)\n",
    "    for sentence in tokenized_sentence:\n",
    "        result.append(get_Name_Entity_Sentence(sentence))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_Name_Entity_StanfordCoreNLP(data):\n",
    "    st = StanfordNERTagger('english.muc.7class.distsim.crf.ser.gz')\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        sentence = sentence.encode('utf8')\n",
    "        tokenized_text = nltk.word_tokenize(sentence)\n",
    "        classified_text = st.tag(tokenized_text)\n",
    "        result = []\n",
    "        for res in classified_text:\n",
    "            if len(res) > 0:\n",
    "                entity_name = res[0]\n",
    "                entity_type = res[1]\n",
    "                if len(entity_name) > 1 and len(entity_type) > 1:\n",
    "                    result.append([entity_name.strip(), entity_type.strip()])\n",
    "        print (sentence, res)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def addNameEntity(df,feature,func):\n",
    "    if 'NE'+\"_\"+feature in df:\n",
    "        df = df.drop('NE'+\"_\"+feature, axis=1)\n",
    "    df[\"NE\"+\"_\"+feature] = func(df[feature])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add named entity of questions to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = addNameEntity(df_training,\"question\",get_Name_Entity_NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add named entity of question answers to training dataframe. (Not needed for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = addNameEntity(df_training,\"text\",get_Name_Entity_StanfordCoreNLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> GET KEYWORDS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "POS = set([\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"CD\",\"JJ\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]) \n",
    "#returns lemmatized word\n",
    "\n",
    "def lemmatize(word):\n",
    "    word = unicode(word, errors='ignore')\n",
    "    lemma = lemmatizer.lemmatize(word,wn.NOUN)\n",
    "    if (lemma == word):\n",
    "        lemma = lemmatizer.lemmatize(word,wn.VERB)\n",
    "        \n",
    "    return lemma\n",
    "\n",
    "def get_keyword(data):\n",
    "    result = []\n",
    "    sentence = data.encode('utf8')\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    for text,pos in tagged:\n",
    "        text = lemmatize(text.lower())\n",
    "        if text not in stopwords.words('english'):\n",
    "            if pos in POS:\n",
    "                result.append(text)\n",
    "                \n",
    "    return result\n",
    "\n",
    "def get_keyword_paragraph(data):\n",
    "    results=[]\n",
    "    tokenized_sentence = nltk.sent_tokenize(data)\n",
    "    for sentence in tokenized_sentence:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_keyword_all(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def add_keywords(df,feature):\n",
    "    if 'keywords'+\"_\"+feature in df:\n",
    "        df = df.drop('keywords'+\"_\"+feature, axis=1)\n",
    "    df['keywords'+\"_\"+feature]=get_keyword_all(df[feature])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "df_training = add_keywords(df_training,'question')\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> GET POS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import StanfordPOSTagger\n",
    "\n",
    "stanford_tagger = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def get_POS_paragraph(data):\n",
    "    result = []\n",
    "    tokenized_sentence = nltk.sent_tokenize(data)\n",
    "    for sentence in tokenized_sentence:\n",
    "        result.append(get_POS(sentence))\n",
    "    return result\n",
    "    \n",
    "def get_POS(data):\n",
    "    tokenized_text = tokenizer.tokenize(data)\n",
    "    nltk_tag = nltk.pos_tag(tokenized_text)\n",
    "    stan_tag = stanford_tagger.tag(tokenized_text)\n",
    "    return stan_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exploring Question Types</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "questionwords = set([\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"whose\", \"which\", \"whom\",\"whats\",\"what's\",\"whos\"])\n",
    "for index,row in df_training.sort_values(by=['question']).iterrows():\n",
    "    sentence = row[\"question\"]\n",
    "    found = False\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    for text in tokenized_text:\n",
    "        if text.lower() in questionwords:\n",
    "            found = True\n",
    "            break;\n",
    "    if (not found):\n",
    "        print (index,sentence,row[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exploring Answer Paragraphs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df_docs.iloc[0]['text']\n",
    "paragraph = doc[22]\n",
    "data = nltk.sent_tokenize(paragraph)\n",
    "for sentence in data:\n",
    "    result = get_Name_Entity_Sentence(sentence)\n",
    "    print (sentence)\n",
    "    print (result)\n",
    "    print (get_keyword(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = doc[22]\n",
    "\n",
    "print (get_keyword_paragraph(paragraph))\n",
    "print (get_POS_paragraph(paragraph))\n",
    "print (get_Name_Entity_paragraph(paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exploring Questions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded word lists\n",
    "questionwords = set([\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"whose\", \"which\", \"whom\",\"whats\",\"what's\",\"whos\"])\n",
    "passiveQuestions = [\"can\", \"could\", \"would\", \n",
    "                    \"was\", \"were\",\"am\",\"is\", \"are\", \"will\",\"shall\",\n",
    "                    \"did\",\"do\",\"does\",\n",
    "                    \"had\", \"have\",\"has\",\n",
    "                    \"as\",\"that\",\"in\",\n",
    "                    \"give an example\",\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in df_training['question'][0:5]:\n",
    "    print (get_Name_Entity_Sentence(question))\n",
    "    print (get_keyword(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df_training.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    answer = row[\"text\"]\n",
    "    print (question,answer.encode('utf-8'))\n",
    "    print (\"*****QUESTION*****\")\n",
    "    print (get_POS(question))\n",
    "    print (get_keyword(question))\n",
    "    print (get_Name_Entity_Sentence(question))\n",
    "    print (\"*****Answer*****\")\n",
    "    print (get_POS(answer))\n",
    "    print (get_keyword(answer))\n",
    "    print (get_Name_Entity_Sentence(answer))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Candidate Answering Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Candidate answering scoring</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Answer and confidence</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
