{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0. Exploratory analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#df_training=pd.read_json('project_files/training.json', encoding = 'utf8')\n",
    "df_devel=pd.read_json('project_files/devel.json')\n",
    "df_docs=pd.read_json('project_files/documents.json')\n",
    "df_testing=pd.read_json('project_files/testing.json')\n",
    "\n",
    "df_training=pd.read_pickle('project_files/df_training.pkl')\n",
    "question_learning_dataset = df_training[df_training.answer_type.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find Keywords\n",
    "2. Answer types - Using answer type taxonomy\n",
    "3. Query formulation -> Keywords\n",
    "4. Go to each document and check the frequency distribution of words and pick the document if one of the query words are present in document. Create a rank with that score\n",
    "5. Find the paragraphs -> Discard irrelevant paragraphs. Use NE,Keywords, longest exact keywords. Put same weight for now and calculate the score of paragraphs. Rank each of the paragraphs in the document. We have to use the original answer and match the answer type\n",
    "6. Find candidate answers -> Use supervised ML method\n",
    "7. Merge candidate answers -> Use NER\n",
    "8. Pick the best answer -> Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Question processing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring Stanford CoreNLP . Link -> https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag.stanford import CoreNLPNERTagger\n",
    "from itertools import groupby\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "\n",
    "\n",
    "def get_Name_Entity_NLTK(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        ne_chunked_sents = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "        result = []\n",
    "\n",
    "        for tagged_tree in ne_chunked_sents:\n",
    "\n",
    "            if hasattr(tagged_tree, 'label'):\n",
    "                entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "                entity_type = tagged_tree.label() # get NE category\n",
    "                result.append((entity_name, entity_type))\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_Name_Entity_Sentence(sentence):\n",
    "    st = CoreNLPNERTagger(url='http://localhost:9000')\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    result = []\n",
    "    \n",
    "    for tag, chunk in groupby(classified_text, lambda x:x[1]):\n",
    "       if tag != \"O\":\n",
    "            word = \" \".join(w for w, t in chunk)\n",
    "            result.append((word.lower(), tag))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_Name_Entity_paragraph(paragraph):\n",
    "    st = CoreNLPNERTagger(url='http://localhost:9000')\n",
    "    entity_para=[]\n",
    "    \n",
    "    tokenized_sentence=nltk.sent_tokenize(paragraph)\n",
    "    #print(tokenized_sentence)\n",
    "    for sentence in tokenized_sentence:\n",
    "        #print(sentence)\n",
    "        tokenized_text = nltk.word_tokenize(sentence)\n",
    "        classified_text = st.tag(tokenized_text)\n",
    "        #result = {}\n",
    "        entity_sent=[]\n",
    "        for tag, chunk in groupby(classified_text, lambda x:x[1]):\n",
    "           if tag != \"O\":\n",
    "                word = \" \".join(w for w, t in chunk)\n",
    "                #result[word.lower()] = tag\n",
    "                entity_sent.append((word.lower(),tag))\n",
    "        #print(entity_sent)     \n",
    "        entity_para.append(entity_sent)\n",
    "        \n",
    "    return entity_para\n",
    "\n",
    "def get_Name_Entity_StanfordCoreNLP(data):\n",
    "    st = CoreNLPNERTagger(url='http://localhost:9000')\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        tokenized_text = nltk.word_tokenize(sentence)\n",
    "        classified_text = st.tag(tokenized_text)\n",
    "        result = []\n",
    "        for tag, chunk in groupby(classified_text, lambda x:x[1]):\n",
    "            if tag != \"O\":\n",
    "                word = \" \".join(w for w, t in chunk)\n",
    "                result.append((word.lower(),tag))\n",
    "       \n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def addNameEntity(df,feature,func):\n",
    "    if 'NE'+\"_\"+feature in df:\n",
    "        df = df.drop('NE'+\"_\"+feature, axis=1)\n",
    "    df[\"NE\"+\"_\"+feature] = func(df[feature])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "POS = set([\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"CD\",\"JJ\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]) \n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,wn.NOUN)\n",
    "    if (lemma == word):\n",
    "        lemma = lemmatizer.lemmatize(word,wn.VERB)\n",
    "        \n",
    "    return lemma\n",
    "\n",
    "def get_keyword(data):\n",
    "    result = []\n",
    "    sentence=data\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    for text,pos in tagged:\n",
    "        text = lemmatize(text.lower())\n",
    "        if text not in stopwords:\n",
    "            if pos in POS:\n",
    "                result.append(text)\n",
    "                \n",
    "    return result\n",
    "\n",
    "def get_keyword_paragraph(data):\n",
    "    results=[]\n",
    "    tokenized_sentence = nltk.sent_tokenize(data)\n",
    "    for sentence in tokenized_sentence:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_keyword_all(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def add_keywords(df,feature):\n",
    "    if 'keywords'+\"_\"+feature in df:\n",
    "        df = df.drop('keywords'+\"_\"+feature, axis=1)\n",
    "    df['keywords'+\"_\"+feature]=get_keyword_all(df[feature])\n",
    "    return df\n",
    "\n",
    "def get_number_of_common_kewyords(question_keywords,answer_sentence_keywords):\n",
    "    sum_keywords=0\n",
    "    for qkey in question_keywords:\n",
    "        if qkey in answer_sentence_keywords:\n",
    "            sum_keywords+=1\n",
    "    \n",
    "    return sum_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train a classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import difflib\n",
    "\n",
    "def isEqual(answer,sentence):\n",
    "    answer_tokens=nltk.word_tokenize(answer)\n",
    "    sentence_tokens=set(nltk.word_tokenize(sentence))\n",
    "    for a_token in answer_tokens:\n",
    "        if a_token not in sentence_tokens:\n",
    "            return False\n",
    "    return True\n",
    "                \n",
    "def get_answer_features(paragraph,answer,ner_answer,ner_paragraph,answer_found):\n",
    "    dict_answer_ner={}\n",
    "    \n",
    "    for ner in ner_answer:\n",
    "        if len(ner)<2:\n",
    "            print ('NER list',ner)\n",
    "        dict_answer_ner[ner[0]]=ner[1]\n",
    "    \n",
    "    dict_answer_sentence_ner={}\n",
    "    ner_paragraph_list=[]\n",
    "    for ner_list in ner_paragraph:\n",
    "        for ner in ner_list:\n",
    "            #if len(ner)<2:\n",
    "             #   print ('NER list',ner_list,ner)\n",
    "            dict_answer_ner[ner[0]]=ner[1]\n",
    "        ner_paragraph_list.append(dict_answer_ner)\n",
    "\n",
    "    \n",
    "    \n",
    "    sents_passage = nltk.sent_tokenize(paragraph)\n",
    "    answer_sentence_ner={'UNKNOWN':'UNKNOWN'}\n",
    "\n",
    "    answer_sentence_keywords=[]\n",
    "    common_entities=tuple()\n",
    "    \n",
    "    \n",
    "    common_entities = set(dict_answer_sentence_ner.items()) & set(dict_answer_ner.items())\n",
    "    \n",
    "    \n",
    "    for sentence_index in range(len(sents_passage)):\n",
    "        #if answer.lower() in sents_passage[sentence_index].lower():\n",
    "        if (isEqual(answer.lower(),sents_passage[sentence_index].lower())):\n",
    "            answer_found=sents_passage[sentence_index]\n",
    "            dict_answer_sentence_ner=ner_paragraph_list[sentence_index]\n",
    "            common_entities = set(dict_answer_sentence_ner.items()) & set(dict_answer_ner.items())\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return answer_found,dict_answer_sentence_ner,common_entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW extraction for passages and questions\n",
    "def get_passages_bow(passages):\n",
    "    passage_bow={}\n",
    "    for passage in passages:\n",
    "        for token in nltk.word_tokenize(passage):\n",
    "            if token not in stopwords: \n",
    "                word=lemmatize(token.lower())\n",
    "                passage_bow[word] = passage_bow.get(word, 0) +  1\n",
    "    \n",
    "    return passage_bow\n",
    "\n",
    "def get_sentences_bow(sentences):\n",
    "    sentence_bow={}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for token in nltk.word_tokenize(sentence):\n",
    "            if token not in stopwords:\n",
    "                word=lemmatize(token.lower())\n",
    "                sentence_bow[word] = sentence_bow.get(word, 0) +  1\n",
    "    \n",
    "    return sentence_bow\n",
    "\n",
    "def get_question_bow(question):\n",
    "    question_bow={}\n",
    "    for token in nltk.word_tokenize(question):\n",
    "        if token not in stopwords: \n",
    "            word=lemmatize(token.lower())\n",
    "            question_bow[word] = question_bow.get(word, 0) +  1\n",
    "                \n",
    "    return question_bow\n",
    "\n",
    "def get_training_question_bow(question,keywords,qt):\n",
    "    question_bow={}\n",
    "    question_bow[qt]=1\n",
    "    for token in nltk.word_tokenize(question):\n",
    "        if token not in stopwords: \n",
    "            word=lemmatize(token.lower())\n",
    "            if word in keywords:\n",
    "                question_bow[word] = question_bow.get(word, 0) +  1\n",
    "                \n",
    "    return question_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_questions(questions, keywords,qt):\n",
    "    qs = []\n",
    "    for i,question in enumerate(questions):\n",
    "        q_bow = get_training_question_bow(question,keywords,qt[i])\n",
    "        qs.append(q_bow)\n",
    "        \n",
    "    return qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def check_results(predictions, classifications):\n",
    "    print(\"Accuracy:\")\n",
    "    print(accuracy_score(classifications,predictions))\n",
    "    print(classification_report(classifications,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "    \n",
    "# get the most common words from answer sentences (we can twek this for paragraph)\n",
    "answer_sentences_bow=get_sentences_bow(question_learning_dataset[question_learning_dataset['answer_found'].notnull()]['answer_found'])\n",
    "answer_keywords = set([word for word, count in answer_sentences_bow.items()])\n",
    "\n",
    "#qs_training=get_feature_questions(questions,answer_keywords)\n",
    "qs_training=get_feature_questions(list(question_learning_dataset.question),answer_keywords,list(question_learning_dataset.question_type))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.8439676567128332\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "   CAUSE_OF_DEATH       1.00      0.50      0.66       327\n",
      "             CITY       1.00      0.08      0.15        12\n",
      "          COUNTRY       0.94      0.55      0.69      1058\n",
      "  CRIMINAL_CHARGE       1.00      0.42      0.59        64\n",
      "             DATE       0.78      0.99      0.87      5801\n",
      "         DURATION       0.97      0.67      0.79       464\n",
      "         IDEOLOGY       0.99      0.58      0.73       232\n",
      "         LOCATION       0.79      0.91      0.85      1738\n",
      "             MISC       1.00      0.54      0.70       133\n",
      "            MONEY       1.00      0.82      0.90       462\n",
      "      NATIONALITY       0.99      0.44      0.61       858\n",
      "           NUMBER       0.93      0.91      0.92      4644\n",
      "          ORDINAL       1.00      0.70      0.82       406\n",
      "     ORGANIZATION       1.00      0.53      0.69       496\n",
      "          PERCENT       0.97      0.87      0.92       751\n",
      "           PERSON       0.79      0.98      0.87      5468\n",
      "         RELIGION       0.99      0.63      0.77       493\n",
      "              SET       1.00      0.44      0.61       174\n",
      "STATE_OR_PROVINCE       1.00      0.47      0.64       335\n",
      "             TIME       1.00      0.42      0.59        53\n",
      "            TITLE       1.00      0.36      0.53      1000\n",
      "              URL       1.00      0.69      0.82        13\n",
      "\n",
      "      avg / total       0.87      0.84      0.83     24982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if (len(qs_training)>0 and len(list(question_learning_dataset.question_type))>0):\n",
    "    # fit vectorizer\n",
    "    vectorizer = DictVectorizer()\n",
    "    \n",
    "    X_train_dtm = vectorizer.fit_transform(qs_training)\n",
    "    \n",
    "    \n",
    "\n",
    "    model=RandomForestClassifier(n_estimators = 300, max_depth = 60, criterion = 'entropy')\n",
    "    \n",
    "    # tag the answers\n",
    "    # fit a logistic regression model to the data \n",
    "    # build classifier\n",
    "    #model = MultinomialNB(2, False, None)\n",
    "\n",
    "    # train the model using X_train_dtm \n",
    "    model.fit(X_train_dtm, list(question_learning_dataset.answer_type))\n",
    "    \n",
    "    y_predicted_class = model.predict(X_train_dtm)\n",
    "    \n",
    "    check_results(y_predicted_class,list(question_learning_dataset.answer_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24982, 13286)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Candidate answering generation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Get a score for the passage to filter the most relevant passages</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## features relevant to this part\n",
    "# number of named entities of the right type in the passage\n",
    "# number of question keywords in the passage\n",
    "# the longest exact sequence of question keywords\n",
    "# rank of the document where the passage was extracted\n",
    "# proximity of the keywords from the original query\n",
    "# ngram overlap between the passage and the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will set up useful functions to extract term frequencies to build the vector space model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "\n",
    "\n",
    "# get the terms for a passage\n",
    "def get_terms(passage):\n",
    "    terms = set()\n",
    "    for token in nltk.word_tokenize(passage):\n",
    "        if token not in stopwords: \n",
    "            terms.add(lemmatize(token.lower()))\n",
    "    return terms\n",
    "    \n",
    "# get document_term \n",
    "def get_document_term_passsages(ds_documents):\n",
    "    document_term={}\n",
    "    passageID=0\n",
    "    for index, row in ds_documents.iterrows():\n",
    "        passageID=0\n",
    "        terms={}\n",
    "        # every row is a document\n",
    "        list_of_passages=row['text']\n",
    "        for passage in list_of_passages:\n",
    "            terms[passageID]=get_terms(passage)\n",
    "            passageID+=1\n",
    "            \n",
    "        document_term[row['docid']]=terms\n",
    "    return document_term\n",
    "\n",
    "# get the term frequency\n",
    "def extract_term_freqs(doc):\n",
    "    tfs = Counter()\n",
    "    for token in doc:\n",
    "        if token not in stopwords: \n",
    "            tfs[lemmatize(token.lower())] += 1\n",
    "    return tfs\n",
    "        \n",
    "# compute idf\n",
    "def compute_doc_freqs(doc_term_freqs):\n",
    "    doc_dic = {}\n",
    "    for key, value in doc_term_freqs.items():\n",
    "        dfs = Counter()\n",
    "        for passage_id,tfs in value.items():\n",
    "            for term in tfs.keys():\n",
    "                dfs[term] += 1\n",
    "        doc_dic[key] = dfs\n",
    "        \n",
    "    return doc_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document-term matrix\n",
    "docs=get_document_term_passsages(df_docs)\n",
    "#docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector space model we need to define a score function\n",
    "# first I will use tf-idf\n",
    "doc_term_freqs = {}\n",
    "for docid,dic_passages in docs.items():\n",
    "    passage_dic = {}\n",
    "    for passage_id, terms in dic_passages.items():\n",
    "        term_freqs = extract_term_freqs(terms)\n",
    "        passage_dic[passage_id] = term_freqs\n",
    "    doc_term_freqs[docid] = passage_dic\n",
    "\n",
    "doc_freqs = compute_doc_freqs(doc_term_freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_term_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Improvement:</b> Use BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inverted index for query processing. Inverted index will not change from query to query. Here we can improve how the weight is defined for the posting list tuple for each term (docid,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(freqs):\n",
    "    p_count=0\n",
    "    for counter in freqs.values():\n",
    "        p_count+=sum(counter.values())\n",
    "    \n",
    "    #print(p_count)\n",
    "    return p_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code from WSTA_N16_information_retrieval\n",
    "vsm_inverted_index_all = defaultdict()\n",
    "for docid, passage_freqs in doc_term_freqs.items():\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    \n",
    "    #N = sum(passage_freqs.values())\n",
    "    N = count_words(passage_freqs)\n",
    "    #print(N,passage_freqs)\n",
    "    for passage_id, term_freqs in passage_freqs.items():\n",
    "        length = 0\n",
    "        # find tf*idf values and accumulate sum of squares \n",
    "        tfidf_values = []\n",
    "        M = len(passage_freqs)\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(doc_freqs[docid][term])) # should be number of documents (paragraphs) with term\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([passage_id, tfidf / length])\n",
    "    vsm_inverted_index_all[docid] = vsm_inverted_index\n",
    "\n",
    "# ensure posting lists are in sorted order (less important here cf above)\n",
    "for key, value in vsm_inverted_index_all.items():\n",
    "    for term, docids in value.items():\n",
    "        docids.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsm_inverted_index_all=load_obj('vsm_inverted_index_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the VSM creating a score for each document (passage) and returning the top k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 0.15652088017686194]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsm_inverted_index_all[0]['addition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# get a list of paragraphs ordered by relevance on the question\n",
    "def query_vsm(query, index):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator\n",
    "\n",
    "##Â end copied code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Candidate answering scoring</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_devel=pd.DataFrame(columns=['id','answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_result_devel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-442-b131afe7f228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m#print('Predicted answer:',answer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mdf_result_devel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_result_devel' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in df_devel.iterrows():\n",
    "    question=row['question']\n",
    "    docid=row['docid']\n",
    "    expected_answer=row['text']\n",
    "    \n",
    "    question_keywords=get_keyword(question)\n",
    "    \n",
    "    # get the most relevant documents for the question\n",
    "    results = query_vsm(question_keywords, vsm_inverted_index_all[docid])\n",
    "    documents_ranked=results.most_common(10) \n",
    "    \n",
    "    # extract a set of potential answers\n",
    "    \n",
    "    q_bow=get_question_bow(question)\n",
    "    x = vectorizer.transform(q_bow)\n",
    "    answer_type=model.predict(x)\n",
    "    #print('Predicted answer type: ',answer_type)\n",
    "    \n",
    "    candidate_passages={}\n",
    "    list_of_passages=[]\n",
    "    answer=''\n",
    "    if len(documents_ranked)>0:\n",
    "        for document in documents_ranked:\n",
    "            # perform a paragraph segmentation\n",
    "            paragraph=df_docs.iloc[docid]['text'][document[0]]\n",
    "            passages = nltk.sent_tokenize(paragraph)\n",
    "            for passage in passages:\n",
    "                list_of_passages.append(passage)\n",
    "\n",
    "\n",
    "\n",
    "        ## PARAMETERS TO GET FROM TESTING DATASET AND USE A MODEL TO GET THE ANSWER PASSAGE CANDIDATES. \n",
    "        #question= df_training.loc[(df_training[\"docid\"] == docid_query) & (df_training[\"answer_paragraph\"] ==document[0] ),\"question\"][0]\n",
    "        #answer_type=df_training.loc[(df_training[\"docid\"] == docid_query) & (df_training[\"answer_paragraph\"] ==document[0] ),\"answer_type\"][0]\n",
    "        #print(question)\n",
    "        #print(answer_type) \n",
    "        #print(sorted(get_keyword(question)))\n",
    "        ###\n",
    "\n",
    "        ## FOR NOW USING KEYWORDS AND GET JUST ONE DEFINITE ANSWER PASSAGE CANDIDATE\n",
    "        indexPassage=0\n",
    "        for indexPassage in range(len(list_of_passages)):\n",
    "            NER_passage=get_Name_Entity_Sentence(list_of_passages[indexPassage])\n",
    "            for entity in NER_passage.items():\n",
    "                if (entity[1]==answer_type):\n",
    "                    candidate_passages[indexPassage]=get_number_of_common_kewyords(get_keyword(question),get_keyword(list_of_passages[indexPassage]))\n",
    "                    break\n",
    "\n",
    "\n",
    "        if len(candidate_passages)>0:\n",
    "            best_candidate_passage=list_of_passages[max(candidate_passages, key=candidate_passages.get)]\n",
    "        else:\n",
    "            if len(list_of_passages)>0:\n",
    "                best_candidate_passage=list_of_passages[0]\n",
    "        #print(\"Candidate Passage Answer:\")\n",
    "        #print(best_candidate_passage)\n",
    "\n",
    "        NER_answer_passage=get_Name_Entity_Sentence(best_candidate_passage)\n",
    "        for entity in NER_answer_passage.items():\n",
    "                if (entity[1]==answer_type):\n",
    "                    answer=entity[0]\n",
    "\n",
    "        #print('Predicted answer:',answer)\n",
    "        df_result_devel.loc[len(df_result)]=[index,answer]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_result=pd.DataFrame(columns=['id','answer'])\n",
    "df_testing=pd.read_json('project_files/testing.json')\n",
    "\n",
    "for index, row in df_testing.iterrows():\n",
    "    question=row['question']\n",
    "    docid=row['docid']\n",
    "    ida=row['id']\n",
    "    \n",
    "    #print('Question: ',question)\n",
    "    #print('Expected Answer:',expected_answer)\n",
    "    #print('Docid:',docid)\n",
    "    question_keywords=get_keyword(question)\n",
    "    \n",
    "    # get the most relevant documents for the question\n",
    "    results = query_vsm(question_keywords, vsm_inverted_index_all[docid])\n",
    "    documents_ranked=results.most_common(10) \n",
    "    #print('Top 10 paragraphs: ',documents_ranked)\n",
    "    q_bow=get_question_bow(question)\n",
    "    x = vectorizer.transform(q_bow)\n",
    "    answer_type=model.predict(x)\n",
    "    #print('Predicted answer type: ',answer_type)\n",
    "    \n",
    "    candidate_passages={}\n",
    "    list_of_passages=[]\n",
    "    answer=''\n",
    "    if len(documents_ranked)>0:\n",
    "        for document in documents_ranked:\n",
    "            # perform a paragraph segmentation\n",
    "            paragraph=df_docs.iloc[docid]['text'][document[0]]\n",
    "            passages = nltk.sent_tokenize(paragraph)\n",
    "            \n",
    "            for passage in passages:\n",
    "                list_of_passages.append(passage)\n",
    "\n",
    "\n",
    "\n",
    "        ## PARAMETERS TO GET FROM TESTING DATASET AND USE A MODEL TO GET THE ANSWER PASSAGE CANDIDATES. \n",
    "        #question= df_training.loc[(df_training[\"docid\"] == docid_query) & (df_training[\"answer_paragraph\"] ==document[0] ),\"question\"][0]\n",
    "        #answer_type=df_training.loc[(df_training[\"docid\"] == docid_query) & (df_training[\"answer_paragraph\"] ==document[0] ),\"answer_type\"][0]\n",
    "        #print(question)\n",
    "        #print(answer_type) \n",
    "        #print(sorted(get_keyword(question)))\n",
    "        ###\n",
    "\n",
    "        ## FOR NOW USING KEYWORDS AND GET JUST ONE DEFINITE ANSWER PASSAGE CANDIDATE\n",
    "        indexPassage=0\n",
    "        for indexPassage in range(len(list_of_passages)):\n",
    "            NER_passage=get_Name_Entity_Sentence(list_of_passages[indexPassage])\n",
    "            for entity in NER_passage:\n",
    "                if (entity[1]==answer_type):\n",
    "                    candidate_passages[indexPassage]=get_number_of_common_kewyords(get_keyword(question),get_keyword(list_of_passages[indexPassage]))\n",
    "                    break\n",
    "\n",
    "\n",
    "        if len(candidate_passages)>0:\n",
    "            best_candidate_passage=list_of_passages[max(candidate_passages, key=candidate_passages.get)]\n",
    "        else:\n",
    "            if len(list_of_passages)>0:\n",
    "                best_candidate_passage=list_of_passages[0]\n",
    "        #print(\"Candidate Passage Answer:\")\n",
    "        #print(best_candidate_passage)\n",
    "\n",
    "       \n",
    "        NER_answer_passage=get_Name_Entity_Sentence(best_candidate_passage)\n",
    "        for entity in NER_answer_passage:\n",
    "                if (entity[1]==answer_type):\n",
    "                    answer=entity[0]\n",
    "    \n",
    "    #print('Predicted answer:',answer)\n",
    "    print(ida)\n",
    "    \n",
    "    df_result.loc[len(df_result)]=[ida,answer]\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('prediction/output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_subsample(x,y,subsample_size=1.0):\n",
    "    \n",
    "    class_xs = []\n",
    "    min_elems = None\n",
    "\n",
    "    for yi in np.unique(y):\n",
    "        elems = x[(y == yi)]\n",
    "        class_xs.append((yi, elems))\n",
    "        if min_elems == None or elems.shape[0] < min_elems:\n",
    "            min_elems = elems.shape[0]\n",
    "\n",
    "    use_elems = min_elems\n",
    "    if subsample_size < 1:\n",
    "        use_elems = int(min_elems*subsample_size)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for ci,this_xs in class_xs:\n",
    "        if len(this_xs) > use_elems:\n",
    "            np.random.shuffle(this_xs)\n",
    "\n",
    "        x_ = this_xs[:use_elems]\n",
    "        y_ = np.empty(use_elems)\n",
    "        y_.fill(ci)\n",
    "\n",
    "        xs.append(x_)\n",
    "        ys.append(y_)\n",
    "\n",
    "    xs = np.concatenate(xs)\n",
    "    ys = np.concatenate(ys)\n",
    "\n",
    "    return xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df_devel=pd.read_json('project_files/devel.json')\n",
    "\n",
    "    \n",
    "def get_answer_rank_features(dataset):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for index, row in dataset.iterrows():\n",
    "        question=row['question']\n",
    "        raw_answer=row['text']\n",
    "\n",
    "        paragraph=df_docs.iloc[row['docid']]['text'][row['answer_paragraph']]\n",
    "        \n",
    "        #answer_found,dict_answer_sentence_ner,common_entities=get_answer_features(paragraph,raw_answer,row['NE_text'],row['NE_paragraph'])\n",
    "\n",
    "        #Â number of named entities in the passage\n",
    "        num_entities=len(common_entities)\n",
    "\n",
    "        # number of question keywords in the passage\n",
    "        question_keywords=get_keyword(question)\n",
    "        answer_passage_keywords=get_keyword(answer_found)\n",
    "        qk_passage=[]\n",
    "        for qk in question_keywords:\n",
    "            if qk in answer_passage_keywords:\n",
    "                qk_passage.append(qk)\n",
    "        num_qkp=len(qk_passage)   \n",
    "\n",
    "        # longest exact sequence of keywords\n",
    "        longest_exact_sequence=0\n",
    "\n",
    "        for i in range(len(question_keywords)):\n",
    "            if i < len(answer_passage_keywords):\n",
    "                if question_keywords[i] in answer_passage_keywords[i]:\n",
    "                    longest_exact_sequence+=1\n",
    "\n",
    "        # rank of the paragraph where the answer sentence was extracted\n",
    "        results = query_vsm(question_keywords, vsm_inverted_index_all[row['docid']])\n",
    "        documents_ranked=results.most_common(10) \n",
    "        rank_of_paragraph=0\n",
    "        for document in documents_ranked:\n",
    "            if (document[0]==row['answer_paragraph']):\n",
    "                break\n",
    "            else:\n",
    "                rank_of_paragraph+=1\n",
    "\n",
    "        #print('Question:',question)\n",
    "        #print('answer:',answer_found)\n",
    "\n",
    "        #print (num_entities,num_qkp,longest_exact_sequence,rank_of_paragraph)\n",
    "\n",
    "        tokenized_sentence = nltk.sent_tokenize(paragraph)\n",
    "        for sentence in tokenized_sentence:\n",
    "            X.append([num_entities,num_qkp,longest_exact_sequence,rank_of_paragraph])\n",
    "            #print(sentence)\n",
    "            if (sentence==answer_found):\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "\n",
    "        #print(Y_train)\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "\n",
    "    \n",
    "def get_answer_rank_features_devel(dataset):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for index, row in dataset.iterrows():\n",
    "        question=row['question']\n",
    "        raw_answer=row['text']\n",
    "        print(index)\n",
    "        paragraph=df_docs.iloc[row['docid']]['text'][row['answer_paragraph']]\n",
    "        \n",
    "        NE_answer=get_Name_Entity_Sentence(raw_answer)\n",
    "        NE_paragraph=get_Name_Entity_paragraph(paragraph)\n",
    "        #print(NE_answer)\n",
    "        answer_found,dict_answer_sentence_ner,common_entities=get_answer_features(paragraph,raw_answer,NE_answer,NE_paragraph)\n",
    "\n",
    "        #Â number of named entities in the passage\n",
    "        num_entities=len(common_entities)\n",
    "\n",
    "        # number of question keywords in the passage\n",
    "        question_keywords=get_keyword(question)\n",
    "        answer_passage_keywords=get_keyword(answer_found)\n",
    "        qk_passage=[]\n",
    "        for qk in question_keywords:\n",
    "            if qk in answer_passage_keywords:\n",
    "                qk_passage.append(qk)\n",
    "        num_qkp=len(qk_passage)   \n",
    "\n",
    "        # longest exact sequence of keywords\n",
    "        longest_exact_sequence=0\n",
    "\n",
    "        for i in range(len(question_keywords)):\n",
    "            if i < len(answer_passage_keywords):\n",
    "                if question_keywords[i] in answer_passage_keywords[i]:\n",
    "                    longest_exact_sequence+=1\n",
    "\n",
    "        # rank of the paragraph where the answer sentence was extracted\n",
    "        results = query_vsm(question_keywords, vsm_inverted_index_all[row['docid']])\n",
    "        documents_ranked=results.most_common(10) \n",
    "        rank_of_paragraph=0\n",
    "        for document in documents_ranked:\n",
    "            if (document[0]==row['answer_paragraph']):\n",
    "                break\n",
    "            else:\n",
    "                rank_of_paragraph+=1\n",
    "\n",
    "        #print('Question:',question)\n",
    "        #print('answer:',answer_found)\n",
    "\n",
    "        #print (num_entities,num_qkp,longest_exact_sequence,rank_of_paragraph)\n",
    "\n",
    "        tokenized_sentence = nltk.sent_tokenize(paragraph)\n",
    "        for sentence in tokenized_sentence:\n",
    "            \n",
    "            X.append([num_entities,num_qkp,longest_exact_sequence,rank_of_paragraph])\n",
    "            if (answer_found in sentence):\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "\n",
    "        #print(Y_train)\n",
    "        \n",
    "    return X,Y    \n",
    "\n",
    "LogReg = LogisticRegression()\n",
    "\n",
    "X_train,Y_train=get_answer_rank_features(df_training)\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, stratify=Y_train, test_size=0.2)\n",
    "\n",
    "\n",
    "#print(X_train)\n",
    "#print(Y_train)\n",
    "LogReg.fit(X_train, Y_train)\n",
    "print('done training')\n",
    "X,Y=get_answer_rank_features_devel(df_devel)\n",
    "\n",
    "\n",
    "y_predicted_class = LogReg.predict(X)\n",
    "\n",
    "\n",
    "classifications=Y\n",
    "predictions=y_predicted_class\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy_score(classifications,predictions))\n",
    "print(classification_report(classifications,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5396"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_passage_features(passage,question,answer_type):\n",
    "    #Â number of named entities in the passage\n",
    "        num_entities=len(common_entities)\n",
    "\n",
    "        # number of question keywords in the passage\n",
    "        question_keywords=get_keyword(question)\n",
    "        answer_passage_keywords=get_keyword(answer_found)\n",
    "        qk_passage=[]\n",
    "        for qk in question_keywords:\n",
    "            if qk in answer_passage_keywords:\n",
    "                qk_passage.append(qk)\n",
    "        num_qkp=len(qk_passage)   \n",
    "\n",
    "        # longest exact sequence of keywords\n",
    "        longest_exact_sequence=0\n",
    "\n",
    "        for i in range(len(question_keywords)):\n",
    "            if i < len(answer_passage_keywords):\n",
    "                if question_keywords[i] in answer_passage_keywords[i]:\n",
    "                    longest_exact_sequence+=1\n",
    "\n",
    "        # rank of the paragraph where the answer sentence was extracted\n",
    "        results = query_vsm(question_keywords, vsm_inverted_index_all[row['docid']])\n",
    "        documents_ranked=results.most_common(10) \n",
    "        rank_of_paragraph=0\n",
    "        for document in documents_ranked:\n",
    "            if (document[0]==row['answer_paragraph']):\n",
    "                break\n",
    "            else:\n",
    "                rank_of_paragraph+=1\n",
    "\n",
    "        return num_entities,num_qkp,longest_exact_sequence,rank_of_paragraph]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_paragraph</th>\n",
       "      <th>docid</th>\n",
       "      <th>question</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>380</td>\n",
       "      <td>On what date did the companies that became the Computing-Tabulating-Recording Company get consolidated?</td>\n",
       "      <td>june 16 , 1911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>380</td>\n",
       "      <td>What percentage of its desktop PCs does IBM plan to install Open Client on to?</td>\n",
       "      <td>5 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>380</td>\n",
       "      <td>What year did IBM hire its first black salesman?</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>380</td>\n",
       "      <td>IBM made an acquisition in 2009, name it.</td>\n",
       "      <td>spss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>380</td>\n",
       "      <td>This IBM invention is known by the acronym UPC, what is the full name?</td>\n",
       "      <td>universal product code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_paragraph  docid  \\\n",
       "0  5                 380     \n",
       "1  22                380     \n",
       "2  16                380     \n",
       "3  4                 380     \n",
       "4  2                 380     \n",
       "\n",
       "                                                                                                  question  \\\n",
       "0  On what date did the companies that became the Computing-Tabulating-Recording Company get consolidated?   \n",
       "1  What percentage of its desktop PCs does IBM plan to install Open Client on to?                            \n",
       "2  What year did IBM hire its first black salesman?                                                          \n",
       "3  IBM made an acquisition in 2009, name it.                                                                 \n",
       "4  This IBM invention is known by the acronym UPC, what is the full name?                                    \n",
       "\n",
       "                     text  \n",
       "0  june 16 , 1911          \n",
       "1  5 %                     \n",
       "2  1946                    \n",
       "3  spss                    \n",
       "4  universal product code  "
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_devel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={'a':1,'b':2,'c':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-427-6a1284577a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_paragraph</th>\n",
       "      <th>docid</th>\n",
       "      <th>question</th>\n",
       "      <th>text</th>\n",
       "      <th>NE_question</th>\n",
       "      <th>NE_text</th>\n",
       "      <th>NE_paragraph</th>\n",
       "      <th>answer_type</th>\n",
       "      <th>keywords_question</th>\n",
       "      <th>question_type</th>\n",
       "      <th>POS_questions</th>\n",
       "      <th>answer_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>A kilogram could be definined as having a Planck constant of what value?</td>\n",
       "      <td>6966662606895999999â 6.62606896Ã10â34 jâs</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(6966662606895999999 â  6.62606896, NUMBER), (10 â 34, NUMBER)]</td>\n",
       "      <td>[[(general, TITLE), (2011, DATE)], [], [(one, NUMBER)], [(7050135639273999999 â  135639274 Ã, NUMBER), (1042, DATE), (6966662606895999999 â  6.62606896, NUMBER), (10 â 34, NUMBER), (â, NUMBER)]]</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>[kilogram, definined, planck, constant, value]</td>\n",
       "      <td>what</td>\n",
       "      <td>[NN, VBN, NNP, NN, NN]</td>\n",
       "      <td>Possible new definitions include \"the mass of a body at rest whose equivalent energy equals the energy of photons whose frequencies sum to 7050135639273999999â 135639274Ã1042 Hz\", or simply \"the kilogram is defined so that the Planck constant equals 6966662606895999999â 6.62606896Ã10â34 Jâs\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the shape of the object that establishes the base unit of the kilogram?</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[], [], [(1889, DATE), (paris, CITY)], [(1889, DATE), (1, NUMBER), (one, NUMBER), (million, NUMBER)], [(one, NUMBER), (current, DATE), (planck, LOCATION)]]</td>\n",
       "      <td>None</td>\n",
       "      <td>[shape, object, establish, base, unit, kilogram]</td>\n",
       "      <td>what</td>\n",
       "      <td>[NN, NN, VBZ, JJ, NN, NN]</td>\n",
       "      <td>The most urgent unit on the list for redefinition is the kilogram, whose value has been fixed for all science (since 1889) by the mass of a small cylinder of platinumâiridium alloy kept in a vault just outside Paris.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>What example is given as another paired relationship of uncertainly related to standard deviation?</td>\n",
       "      <td>time vs. energy</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[], [], [(one, NUMBER)], [(fourier, LOCATION)]]</td>\n",
       "      <td>None</td>\n",
       "      <td>[example, give, pair, relationship, relate, standard, deviation]</td>\n",
       "      <td>what</td>\n",
       "      <td>[NN, VBN, JJ, NN, VBN, JJ, NN]</td>\n",
       "      <td>One example is time vs. energy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>What does the Planck Constant refer to?</td>\n",
       "      <td>quantum of action</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[], [(planck, PERSON)], [(now, DATE)], [], []]</td>\n",
       "      <td>None</td>\n",
       "      <td>[doe, planck, constant, refer]</td>\n",
       "      <td>what</td>\n",
       "      <td>[VBZ, NNP, NNP, NN]</td>\n",
       "      <td>Instead, it must be some multiple of a very small quantity, the \"quantum of action\", now called the Planck constant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>When was the first quantized model of the atom introduced?</td>\n",
       "      <td>1913</td>\n",
       "      <td>[(first, ORDINAL), (model, TITLE)]</td>\n",
       "      <td>[(1913, DATE)]</td>\n",
       "      <td>[[(niels bohr, PERSON), (first, ORDINAL), (model, TITLE), (1913, DATE), (rutherford, PERSON), (model, TITLE)], [], [], [(bohr, PERSON), (planck, PERSON), (bohr, PERSON)]]</td>\n",
       "      <td>DATE</td>\n",
       "      <td>[wa, first, quantize, model, atom, introduce]</td>\n",
       "      <td>when</td>\n",
       "      <td>[VBD, JJ, JJ, NN, NN, VBD]</td>\n",
       "      <td>Niels Bohr introduced the first quantized model of the atom in 1913, in an attempt to overcome a major shortcoming of Rutherford's classical model.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_paragraph  docid  \\\n",
       "0  23                0       \n",
       "1  22                0       \n",
       "2  12                0       \n",
       "3  1                 0       \n",
       "4  10                0       \n",
       "\n",
       "                                                                                             question  \\\n",
       "0  A kilogram could be definined as having a Planck constant of what value?                             \n",
       "1  What is the shape of the object that establishes the base unit of the kilogram?                      \n",
       "2  What example is given as another paired relationship of uncertainly related to standard deviation?   \n",
       "3  What does the Planck Constant refer to?                                                              \n",
       "4  When was the first quantized model of the atom introduced?                                           \n",
       "\n",
       "                                       text  \\\n",
       "0  6966662606895999999â 6.62606896Ã10â34 jâs   \n",
       "1  cylinder                                   \n",
       "2  time vs. energy                            \n",
       "3  quantum of action                          \n",
       "4  1913                                       \n",
       "\n",
       "                          NE_question  \\\n",
       "0  []                                   \n",
       "1  []                                   \n",
       "2  []                                   \n",
       "3  []                                   \n",
       "4  [(first, ORDINAL), (model, TITLE)]   \n",
       "\n",
       "                                                           NE_text  \\\n",
       "0  [(6966662606895999999 â  6.62606896, NUMBER), (10 â 34, NUMBER)]   \n",
       "1  []                                                                \n",
       "2  []                                                                \n",
       "3  []                                                                \n",
       "4  [(1913, DATE)]                                                    \n",
       "\n",
       "                                                                                                                                                                                       NE_paragraph  \\\n",
       "0  [[(general, TITLE), (2011, DATE)], [], [(one, NUMBER)], [(7050135639273999999 â  135639274 Ã, NUMBER), (1042, DATE), (6966662606895999999 â  6.62606896, NUMBER), (10 â 34, NUMBER), (â, NUMBER)]]   \n",
       "1  [[], [], [(1889, DATE), (paris, CITY)], [(1889, DATE), (1, NUMBER), (one, NUMBER), (million, NUMBER)], [(one, NUMBER), (current, DATE), (planck, LOCATION)]]                                       \n",
       "2  [[], [], [(one, NUMBER)], [(fourier, LOCATION)]]                                                                                                                                                   \n",
       "3  [[], [(planck, PERSON)], [(now, DATE)], [], []]                                                                                                                                                    \n",
       "4  [[(niels bohr, PERSON), (first, ORDINAL), (model, TITLE), (1913, DATE), (rutherford, PERSON), (model, TITLE)], [], [], [(bohr, PERSON), (planck, PERSON), (bohr, PERSON)]]                         \n",
       "\n",
       "  answer_type  \\\n",
       "0  NUMBER       \n",
       "1  None         \n",
       "2  None         \n",
       "3  None         \n",
       "4  DATE         \n",
       "\n",
       "                                                  keywords_question  \\\n",
       "0  [kilogram, definined, planck, constant, value]                     \n",
       "1  [shape, object, establish, base, unit, kilogram]                   \n",
       "2  [example, give, pair, relationship, relate, standard, deviation]   \n",
       "3  [doe, planck, constant, refer]                                     \n",
       "4  [wa, first, quantize, model, atom, introduce]                      \n",
       "\n",
       "  question_type                   POS_questions  \\\n",
       "0  what          [NN, VBN, NNP, NN, NN]           \n",
       "1  what          [NN, NN, VBZ, JJ, NN, NN]        \n",
       "2  what          [NN, VBN, JJ, NN, VBN, JJ, NN]   \n",
       "3  what          [VBZ, NNP, NNP, NN]              \n",
       "4  when          [VBD, JJ, JJ, NN, NN, VBD]       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                          answer_found  \n",
       "0  Possible new definitions include \"the mass of a body at rest whose equivalent energy equals the energy of photons whose frequencies sum to 7050135639273999999â 135639274Ã1042 Hz\", or simply \"the kilogram is defined so that the Planck constant equals 6966662606895999999â 6.62606896Ã10â34 Jâs\".  \n",
       "1  The most urgent unit on the list for redefinition is the kilogram, whose value has been fixed for all science (since 1889) by the mass of a small cylinder of platinumâiridium alloy kept in a vault just outside Paris.                                                                             \n",
       "2  One example is time vs. energy.                                                                                                                                                                                                                                                                      \n",
       "3  Instead, it must be some multiple of a very small quantity, the \"quantum of action\", now called the Planck constant.                                                                                                                                                                                 \n",
       "4  Niels Bohr introduced the first quantized model of the atom in 1913, in an attempt to overcome a major shortcoming of Rutherford's classical model.                                                                                                                                                  "
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2743"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_training[df_training.answer_found=='UNKNOWN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer_paragraph     19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "docid                28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "question             What percent of Oklahomans speak only English at home, as of 2000?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "text                 92.6 %                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "NE_question          [(oklahomans, MISC), (english, NATIONALITY), (2000, DATE)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
       "NE_text              [(92.6 %, PERCENT)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "NE_paragraph         [[(english, NATIONALITY), (oklahoma, STATE_OR_PROVINCE), (2010, DATE)], [(north, MISC), (american english, NATIONALITY), (oklahoma, STATE_OR_PROVINCE), (english, NATIONALITY), (north, LOCATION), (midland, CITY), (south, LOCATION), (midland, CITY), (southern, MISC)], [(2000, DATE), (2,977,187, NUMBER), (oklahomans, MISC), (92.6 %, PERCENT), (five years, DURATION), (english, NATIONALITY), (95 %, PERCENT), (1990, DATE)], [(238,732, NUMBER), (oklahoma, STATE_OR_PROVINCE), (english, NATIONALITY), (2000, DATE), (7.4 %, PERCENT)], [(spanish, NATIONALITY), (second, ORDINAL), (141,060, NUMBER), (2000, DATE)], [(22,000, NUMBER), (oklahoma, STATE_OR_PROVINCE)], [(cherokee, MISC), (cherokee nation, ORGANIZATION), (united keetoowah band of cherokee indians, ORGANIZATION)]]\n",
       "answer_type          PERCENT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "keywords_question    [percent, oklahoman, speak, english, home, 2000]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "question_type        what                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "POS_questions        [NN, NNP, VBP, NNP, NN, CD]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "answer_found         UNKNOWN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "Name: 3105, dtype: object"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.iloc[3105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a='The winner of the 2014 Nobel Prize in Literature, Patrick Modianoâwho lives in Parisâ, based most of his literary work on the depiction of the city during World War II and the 1960s-1970s.'\n",
    "\n",
    "if 'patrick modiano' in a.lower():\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
