{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0. Exploratory analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "#df_training=pd.read_json('project_files/training.json', encoding = 'utf8')\n",
    "df_devel=pd.read_json('project_files/devel.json')\n",
    "df_docs=pd.read_json('project_files/documents.json')\n",
    "df_testing=pd.read_json('project_files/testing.json')\n",
    "\n",
    "df_training=pd.read_pickle('project_files/df_training.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find Keywords\n",
    "2. Answer types - Using answer type taxonomy\n",
    "3. Query formulation -> Keywords\n",
    "4. Go to each document and check the frequency distribution of words and pick the document if one of the query words are present in document. Create a rank with that score\n",
    "5. Find the paragraphs -> Discard irrelevant paragraphs. Use NE,Keywords, longest exact keywords. Put same weight for now and calculate the score of paragraphs. Rank each of the paragraphs in the document. We have to use the original answer and match the answer type\n",
    "6. Find candidate answers -> Use supervised ML method\n",
    "7. Merge candidate answers -> Use NER\n",
    "8. Pick the best answer -> Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Question processing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring Stanford CoreNLP . Link -> https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag.stanford import CoreNLPNERTagger\n",
    "from itertools import groupby\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def get_Name_Entity_NLTK(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        ne_chunked_sents = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "        result = []\n",
    "\n",
    "        for tagged_tree in ne_chunked_sents:\n",
    "\n",
    "            if hasattr(tagged_tree, 'label'):\n",
    "                entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "                entity_type = tagged_tree.label() # get NE category\n",
    "                result.append((entity_name, entity_type))\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_Name_Entity_Sentence(sentence):\n",
    "    st = CoreNLPNERTagger(url='http://localhost:9000')\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    result = {}\n",
    "    \n",
    "    for res in classified_text:\n",
    "        if len(res) > 0:\n",
    "            for tag, chunk in groupby(classified_text, lambda x:x[1]):\n",
    "               if tag != \"O\":\n",
    "                    word = \" \".join(w for w, t in chunk)\n",
    "                    result[word.lower()] = tag\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_Name_Entity_paragraph(paragraph):\n",
    "    result = []\n",
    "    tokenized_sentence = nltk.sent_tokenize(paragraph)\n",
    "    for sentence in tokenized_sentence:\n",
    "        result.append(get_Name_Entity_Sentence(sentence))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_Name_Entity_StanfordCoreNLP(data):\n",
    "    st = CoreNLPNERTagger(url='http://localhost:9000')\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        tokenized_text = nltk.word_tokenize(sentence)\n",
    "        classified_text = st.tag(tokenized_text)\n",
    "        result = []\n",
    "        for tag, chunk in groupby(classified_text, lambda x:x[1]):\n",
    "            if tag != \"O\":\n",
    "                word = \" \".join(w for w, t in chunk)\n",
    "                result.append((word.lower(),tag))\n",
    "       \n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def addNameEntity(df,feature,func):\n",
    "    if 'NE'+\"_\"+feature in df:\n",
    "        df = df.drop('NE'+\"_\"+feature, axis=1)\n",
    "    df[\"NE\"+\"_\"+feature] = func(df[feature])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_training = addNameEntity(df_training,\"question\",get_Name_Entity_StanfordCoreNLP)\n",
    "#df_training = addNameEntity(df_training,\"text\",get_Name_Entity_StanfordCoreNLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "POS = set([\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"CD\",\"JJ\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]) \n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,wn.NOUN)\n",
    "    if (lemma == word):\n",
    "        lemma = lemmatizer.lemmatize(word,wn.VERB)\n",
    "        \n",
    "    return lemma\n",
    "\n",
    "def get_keyword(data):\n",
    "    result = []\n",
    "    sentence=data\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    for text,pos in tagged:\n",
    "        text = lemmatize(text.lower())\n",
    "        if text not in stopwords:\n",
    "            if pos in POS:\n",
    "                result.append(text)\n",
    "                \n",
    "    return result\n",
    "\n",
    "def get_keyword_paragraph(data):\n",
    "    results=[]\n",
    "    tokenized_sentence = nltk.sent_tokenize(data)\n",
    "    for sentence in tokenized_sentence:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_keyword_all(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def add_keywords(df,feature):\n",
    "    if 'keywords'+\"_\"+feature in df:\n",
    "        df = df.drop('keywords'+\"_\"+feature, axis=1)\n",
    "    df['keywords'+\"_\"+feature]=get_keyword_all(df[feature])\n",
    "    return df\n",
    "\n",
    "def get_number_of_common_kewyords(question_keywords,answer_sentence_keywords):\n",
    "    sum_keywords=0\n",
    "    for qkey in question_keywords:\n",
    "        if qkey in answer_sentence_keywords:\n",
    "            sum_keywords+=1\n",
    "    \n",
    "    return sum_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train a classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def get_answer_features(paragraph,answer,ner_answer,ner_paragraph):\n",
    "    dict_answer_ner={}\n",
    "    for ner in ner_answer:\n",
    "        dict_answer_ner[ner[0]]=ner[1]\n",
    "    \n",
    "    dict_answer_sentence_ner={}\n",
    "    ner_paragraph_list=[]\n",
    "    for ner_list in ner_paragraph:\n",
    "        for ner in ner_list:\n",
    "            dict_answer_ner[ner[0]]=ner[1]\n",
    "        ner_paragraph_list.append(dict_answer_ner)\n",
    "            \n",
    "    \n",
    "    sents_passage = nltk.sent_tokenize(paragraph)\n",
    "    answer_sentence_ner={'UNKNOWN':'UNKNOWN'}\n",
    "    answer_found='UNKNOWN'\n",
    "    answer_sentence_keywords=[]\n",
    "    common_entities=tuple()\n",
    "    for sentence_index in range(len(sents_passage)):\n",
    "        if answer.lower() in sents_passage[sentence_index].lower():\n",
    "            answer_found=sents_passage[sentence_index]\n",
    "            dict_answer_sentence_ner=ner_paragraph_list[sentence_index]\n",
    "            common_entities = set(dict_answer_sentence_ner.items()) & set(dict_answer_ner.items())\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return answer_found,dict_answer_sentence_ner,common_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# get the training data set as we need it\n",
    "answer_type={}\n",
    "def prepare_training_data():\n",
    "    \n",
    "    #df_training['answer_type']='UNKNOWN'\n",
    "    df_training['answer_found']='UNKNOWN'\n",
    "    #df_training['common_entities']='UNKNOWN'\n",
    "    #df_training['answer_sentence_ner']='UNKNOWN'\n",
    "    for index, row in df_training.iterrows():\n",
    "        question=row['question']\n",
    "        raw_answer=row['text']\n",
    "        \n",
    "        paragraph=df_docs.iloc[row['docid']]['text'][row['answer_paragraph']]\n",
    "        \n",
    "        answer_found,dict_answer_sentence_ner,common_entities=get_answer_features(paragraph,raw_answer,row['NE_text'],row['NE_paragraph'])\n",
    "        \n",
    "        #if (len(common_entities)>0):\n",
    "        #    answer_type=list(common_entities)[0][1]\n",
    "            \n",
    "        #else:\n",
    "        #    answer_type='UNKNOWN'\n",
    "            \n",
    "        #df_training.at[index,'answer_type']=answer_type\n",
    "        df_training.at[index,'answer_found']=answer_found\n",
    "        #df_training.at[index,'answer_sentence_ner']=dict_answer_sentence_ner\n",
    "        #df_training.at[index,'common_entities']=common_entities\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', -1)\n",
    "#df_training[['question','text','answer_found','answer_sentence_ner','answer_type','NE_text','NE_question','common_entities']][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW extraction for passages and questions\n",
    "\n",
    "\n",
    "def get_passages_bow(passages):\n",
    "    passage_bow={}\n",
    "    for passage in passages:\n",
    "        for token in nltk.word_tokenize(passage):\n",
    "            if token not in stopwords: \n",
    "                word=stemmer.stem(token.lower())\n",
    "                passage_bow[word] = passage_bow.get(word, 0) +  1\n",
    "    \n",
    "    return passage_bow\n",
    "\n",
    "def get_sentences_bow(sentences):\n",
    "    sentence_bow={}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for token in nltk.word_tokenize(sentence):\n",
    "            if token not in stopwords:\n",
    "                word=stemmer.stem(token.lower())\n",
    "                sentence_bow[word] = sentence_bow.get(word, 0) +  1\n",
    "    \n",
    "    return sentence_bow\n",
    "\n",
    "def get_question_bow(question):\n",
    "    question_bow={}\n",
    "    for token in nltk.word_tokenize(question):\n",
    "        if token not in stopwords: \n",
    "            word=stemmer.stem(token.lower())\n",
    "            question_bow[word] = question_bow.get(word, 0) +  1\n",
    "                \n",
    "    return question_bow\n",
    "\n",
    "def get_training_question_bow(question,keywords,qt):\n",
    "    question_bow={}\n",
    "    question_bow[qt]=1\n",
    "    for token in nltk.word_tokenize(question):\n",
    "        if token not in stopwords: \n",
    "            word=stemmer.stem(token.lower())\n",
    "            if word in keywords:\n",
    "                question_bow[word] = question_bow.get(word, 0) +  1\n",
    "                \n",
    "    return question_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NUMBER', 'DATE', 'PERSON', 'IDEOLOGY', 'MISC', 'RELIGION',\n",
       "       'UNKNOWN', 'ORGANIZATION', 'LOCATION', 'COUNTRY', 'TITLE',\n",
       "       'PERCENT', 'DURATION', 'MONEY', 'CAUSE_OF_DEATH',\n",
       "       'STATE_OR_PROVINCE', 'CITY', 'NATIONALITY', 'ORDINAL',\n",
       "       'CRIMINAL_CHARGE', 'TIME', 'SET'], dtype=object)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.answer_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_questions():\n",
    "    questions=[]\n",
    "    questions_not_found=[]\n",
    "    answers_not_found=[]\n",
    "    classes=[]\n",
    "    for index, row in df_training.iterrows():\n",
    "        if len(row['common_entities'])>0 and row['answer_type']!='UNKNOWN':\n",
    "            questions.append(row['question'])\n",
    "            classes.append(row['answer_type'])\n",
    "        else:\n",
    "            questions_not_found.append(row['question'])\n",
    "            answers_not_found.append(row['text'])\n",
    "            \n",
    "    return questions,classes,questions_not_found,answers_not_found\n",
    "        \n",
    "\n",
    "def get_feature_questions(questions, keywords,qt):\n",
    "    qs = []\n",
    "    for i,question in enumerate(questions):\n",
    "        q_bow = get_training_question_bow(question,keywords,qt[i])\n",
    "        qs.append(q_bow)\n",
    "        \n",
    "    return qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24982\n"
     ]
    }
   ],
   "source": [
    "question_learning_dataset = df_training[df_training.answer_type.notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def check_results(predictions, classifications):\n",
    "    print(\"Accuracy:\")\n",
    "    print(accuracy_score(classifications,predictions))\n",
    "    print(classification_report(classifications,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "    \n",
    "# get the most common words from answer sentences (we can twek this for paragraph)\n",
    "answer_sentences_bow=get_sentences_bow(df_training['answer_found'])\n",
    "answer_keywords = set([word for word, count in answer_sentences_bow.items()])\n",
    "\n",
    "#filter questions with common entities\n",
    "#questions,classes,questions_not_found,answers_not_found=filter_questions()\n",
    "\n",
    "\n",
    "#qs_training=get_feature_questions(questions,answer_keywords)\n",
    "qs_training=get_feature_questions(list(question_learning_dataset.question),answer_keywords,list(question_learning_dataset.question_type))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.7229205027619886\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "   CAUSE_OF_DEATH       0.93      0.08      0.15       327\n",
      "             CITY       0.00      0.00      0.00        12\n",
      "          COUNTRY       0.80      0.39      0.52      1058\n",
      "  CRIMINAL_CHARGE       0.00      0.00      0.00        64\n",
      "             DATE       0.69      0.97      0.80      5801\n",
      "         DURATION       0.94      0.16      0.28       464\n",
      "         IDEOLOGY       1.00      0.06      0.11       232\n",
      "         LOCATION       0.70      0.85      0.77      1738\n",
      "             MISC       1.00      0.01      0.01       133\n",
      "            MONEY       0.98      0.21      0.34       462\n",
      "      NATIONALITY       0.73      0.26      0.39       858\n",
      "           NUMBER       0.72      0.89      0.79      4644\n",
      "          ORDINAL       0.93      0.07      0.12       406\n",
      "     ORGANIZATION       0.88      0.20      0.33       496\n",
      "          PERCENT       0.85      0.55      0.67       751\n",
      "           PERSON       0.75      0.92      0.83      5468\n",
      "         RELIGION       0.88      0.35      0.50       493\n",
      "              SET       0.00      0.00      0.00       174\n",
      "STATE_OR_PROVINCE       0.50      0.00      0.01       335\n",
      "             TIME       0.00      0.00      0.00        53\n",
      "            TITLE       0.82      0.21      0.34      1000\n",
      "              URL       0.00      0.00      0.00        13\n",
      "\n",
      "      avg / total       0.74      0.72      0.67     24982\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielgil/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "if (len(qs)>0 and len(classes)>0):\n",
    "    # fit vectorizer\n",
    "    vectorizer = DictVectorizer()\n",
    "    \n",
    "    X_train_dtm = vectorizer.fit_transform(qs_training)\n",
    "    \n",
    "    # tag the answers\n",
    "    # fit a logistic regression model to the data \n",
    "    # build classifier\n",
    "    model = MultinomialNB(2, False, None)\n",
    "\n",
    "    # train the model using X_train_dtm \n",
    "    model.fit(X_train_dtm, list(question_learning_dataset.answer_type))\n",
    "    \n",
    "    y_predicted_class = model.predict(X_train_dtm)\n",
    "    \n",
    "    check_results(y_predicted_class,list(question_learning_dataset.answer_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24982, 13435)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Candidate answering generation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Get a score for the passage to filter the most relevant passages</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "## features relevant to this part\n",
    "# number of named entities of the right type in the passage\n",
    "# number of question keywords in the passage\n",
    "# the longest exact sequence of question keywords\n",
    "# rank of the document where the passage was extracted\n",
    "# proximity of the keywords from the original query\n",
    "# ngram overlap between the passage and the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will set up useful functions to extract term frequencies to build the vector space model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "# get the terms for a passage\n",
    "def get_terms(passage):\n",
    "    terms = set()\n",
    "    for token in nltk.word_tokenize(passage):\n",
    "        if token not in stopwords: \n",
    "            terms.add(stemmer.stem(token.lower()))\n",
    "    return terms\n",
    "    \n",
    "# get document_term \n",
    "def get_document_term_passsages(ds_documents):\n",
    "    document_term={}\n",
    "    passageID=0\n",
    "    for index, row in ds_documents.iterrows():\n",
    "        passageID=0\n",
    "        terms={}\n",
    "        # every row is a document\n",
    "        list_of_passages=row['text']\n",
    "        for passage in list_of_passages:\n",
    "            terms[passageID]=get_terms(passage)\n",
    "            passageID+=1\n",
    "            \n",
    "        document_term[row['docid']]=terms\n",
    "    return document_term\n",
    "\n",
    "# get the term frequency\n",
    "def extract_term_freqs(doc):\n",
    "    tfs = Counter()\n",
    "    for token in doc:\n",
    "        if token not in stopwords: \n",
    "            tfs[stemmer.stem(token.lower())] += 1\n",
    "    return tfs\n",
    "        \n",
    "# compute idf\n",
    "def compute_doc_freqs(doc_term_freqs):\n",
    "    doc_dic = {}\n",
    "    for key, value in doc_term_freqs.items():\n",
    "        dfs = Counter()\n",
    "        for passage_id,tfs in value.items():\n",
    "            for term in tfs.keys():\n",
    "                dfs[term] += 1\n",
    "        doc_dic[key] = dfs\n",
    "        \n",
    "    return doc_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document-term matrix\n",
    "docs=get_document_term_passsages(df_docs)\n",
    "#docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector space model we need to define a score function\n",
    "# first I will use tf-idf\n",
    "doc_term_freqs = {}\n",
    "for docid,dic_passages in docs.items():\n",
    "    passage_dic = {}\n",
    "    for passage_id, terms in dic_passages.items():\n",
    "        term_freqs = extract_term_freqs(terms)\n",
    "        passage_dic[passage_id] = term_freqs\n",
    "    doc_term_freqs[docid] = passage_dic\n",
    "\n",
    "doc_freqs = compute_doc_freqs(doc_term_freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_term_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Improvement:</b> Use BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inverted index for query processing. Inverted index will not change from query to query. Here we can improve how the weight is defined for the posting list tuple for each term (docid,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(freqs):\n",
    "    p_count=0\n",
    "    for counter in freqs.values():\n",
    "        p_count+=sum(counter.values())\n",
    "    \n",
    "    #print(p_count)\n",
    "    return p_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code from WSTA_N16_information_retrieval\n",
    "vsm_inverted_index_all = defaultdict()\n",
    "for docid, passage_freqs in doc_term_freqs.items():\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    \n",
    "    #N = sum(passage_freqs.values())\n",
    "    N = count_words(passage_freqs)\n",
    "    #print(N,passage_freqs)\n",
    "    for passage_id, term_freqs in passage_freqs.items():\n",
    "        length = 0\n",
    "        # find tf*idf values and accumulate sum of squares \n",
    "        tfidf_values = []\n",
    "        M = len(passage_freqs)\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(doc_freqs[docid][term]))\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([passage_id, tfidf / length])\n",
    "    vsm_inverted_index_all[docid] = vsm_inverted_index\n",
    "\n",
    "# ensure posting lists are in sorted order (less important here cf above)\n",
    "for key, value in vsm_inverted_index_all.items():\n",
    "    for term, docids in value.items():\n",
    "        docids.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "save_obj(vsm_inverted_index_all,'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the VSM creating a score for each document (passage) and returning the top k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of paragraphs ordered by relevance on the question\n",
    "def query_vsm(query, index):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator\n",
    "\n",
    "##Â end copied code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Candidate answering scoring</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result=pd.DataFrame(columns=['id','answer'])\n",
    "df_devel=df_devel.iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, row in df_devel.iterrows():\n",
    "    question=row['question']\n",
    "    docid=row['docid']\n",
    "    expected_answer=row['text']\n",
    "    #print('Question: ',question)\n",
    "    #print('Expected Answer:',expected_answer)\n",
    "    #print('Docid:',docid)\n",
    "    question_keywords=get_keyword(question)\n",
    "    \n",
    "    # get the most relevant documents for the question\n",
    "    results = query_vsm(question_keywords, vsm_inverted_index_all[docid])\n",
    "    documents_ranked=results.most_common(10) \n",
    "    #print('Top 10 paragraphs: ',documents_ranked)\n",
    "    q_bow=get_question_bow(question)\n",
    "    x = vectorizer.transform(q_bow)\n",
    "    answer_type=model.predict(x)\n",
    "    #print('Predicted answer type: ',answer_type)\n",
    "    \n",
    "    candidate_passages={}\n",
    "    list_of_passages=[]\n",
    "    answer=''\n",
    "    if len(documents_ranked)>0:\n",
    "        for document in documents_ranked:\n",
    "            # perform a paragraph segmentation\n",
    "            paragraph=df_docs.iloc[docid]['text'][document[0]]\n",
    "            passages = nltk.sent_tokenize(paragraph)\n",
    "            for passage in passages:\n",
    "                list_of_passages.append(passage)\n",
    "\n",
    "\n",
    "\n",
    "        ## PARAMETERS TO GET FROM TESTING DATASET AND USE A MODEL TO GET THE ANSWER PASSAGE CANDIDATES. \n",
    "        #question= df_training.loc[(df_training[\"docid\"] == docid_query) & (df_training[\"answer_paragraph\"] ==document[0] ),\"question\"][0]\n",
    "        #answer_type=df_training.loc[(df_training[\"docid\"] == docid_query) & (df_training[\"answer_paragraph\"] ==document[0] ),\"answer_type\"][0]\n",
    "        #print(question)\n",
    "        #print(answer_type) \n",
    "        #print(sorted(get_keyword(question)))\n",
    "        ###\n",
    "\n",
    "        ## FOR NOW USING KEYWORDS AND GET JUST ONE DEFINITE ANSWER PASSAGE CANDIDATE\n",
    "        indexPassage=0\n",
    "        for indexPassage in range(len(list_of_passages)):\n",
    "            NER_passage=get_Name_Entity_Sentence(list_of_passages[indexPassage])\n",
    "            for entity in NER_passage.items():\n",
    "                if (entity[1]==answer_type):\n",
    "                    candidate_passages[indexPassage]=get_number_of_common_kewyords(get_keyword(question),get_keyword(list_of_passages[indexPassage]))\n",
    "                    break\n",
    "\n",
    "\n",
    "        if len(candidate_passages)>0:\n",
    "            best_candidate_passage=list_of_passages[max(candidate_passages, key=candidate_passages.get)]\n",
    "        else:\n",
    "            if len(list_of_passages)>0:\n",
    "                best_candidate_passage=list_of_passages[0]\n",
    "        #print(\"Candidate Passage Answer:\")\n",
    "        #print(best_candidate_passage)\n",
    "\n",
    "        NER_answer_passage=get_Name_Entity_Sentence(best_candidate_passage)\n",
    "        for entity in NER_answer_passage.items():\n",
    "                if (entity[1]==answer_type):\n",
    "                    answer=entity[0]\n",
    "\n",
    "        #print('Predicted answer:',answer)\n",
    "        df_result.loc[len(df_result)]=[index,answer]\n",
    "    break\n",
    "    \n",
    "df_result.to_csv('prediction/output.csv',index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "      <td>Modern browser support standards-based and defacto what?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>410</td>\n",
       "      <td>1</td>\n",
       "      <td>What do people typically call a web browser?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>410</td>\n",
       "      <td>2</td>\n",
       "      <td>What is it called when content is changed from markup to an interactive document?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>410</td>\n",
       "      <td>3</td>\n",
       "      <td>What platform is a browser used on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>410</td>\n",
       "      <td>4</td>\n",
       "      <td>When was Firefox released?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   docid  id  \\\n",
       "0  410    0    \n",
       "1  410    1    \n",
       "2  410    2    \n",
       "3  410    3    \n",
       "4  410    4    \n",
       "\n",
       "                                                                            question  \n",
       "0  Modern browser support standards-based and defacto what?                           \n",
       "1  What do people typically call a web browser?                                       \n",
       "2  What is it called when content is changed from markup to an interactive document?  \n",
       "3  What platform is a browser used on?                                                \n",
       "4  When was Firefox released?                                                         "
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_testing.iterrows():\n",
    "    question=row['question']\n",
    "    docid=row['docid']\n",
    "    ida=row['id']\n",
    "    #print('Question: ',question)\n",
    "    #print('Expected Answer:',expected_answer)\n",
    "    #print('Docid:',docid)\n",
    "    question_keywords=get_keyword(question)\n",
    "    \n",
    "    # get the most relevant documents for the question\n",
    "    results = query_vsm(question_keywords, vsm_inverted_index_all[docid])\n",
    "    documents_ranked=results.most_common(10) \n",
    "    #print('Top 10 paragraphs: ',documents_ranked)\n",
    "    q_bow=get_question_bow(question)\n",
    "    x = vectorizer.transform(q_bow)\n",
    "    answer_type=model.predict(x)\n",
    "    #print('Predicted answer type: ',answer_type)\n",
    "    \n",
    "    candidate_passages={}\n",
    "    list_of_passages=[]\n",
    "    answer=''\n",
    "    if len(documents_ranked)>0:\n",
    "        for document in documents_ranked:\n",
    "            # perform a paragraph segmentation\n",
    "            paragraph=df_docs.iloc[docid]['text'][document[0]]\n",
    "            passages = nltk.sent_tokenize(paragraph)\n",
    "            \n",
    "            for passage in passages:\n",
    "                list_of_passages.append(passage)\n",
    "\n",
    "\n",
    "\n",
    "        ## PARAMETERS TO GET FROM TESTING DATASET AND USE A MODEL TO GET THE ANSWER PASSAGE CANDIDATES. \n",
    "        #question= df_training.loc[(df_training[\"docid\"] == docid_query) & (df_training[\"answer_paragraph\"] ==document[0] ),\"question\"][0]\n",
    "        #answer_type=df_training.loc[(df_training[\"docid\"] == docid_query) & (df_training[\"answer_paragraph\"] ==document[0] ),\"answer_type\"][0]\n",
    "        #print(question)\n",
    "        #print(answer_type) \n",
    "        #print(sorted(get_keyword(question)))\n",
    "        ###\n",
    "\n",
    "        ## FOR NOW USING KEYWORDS AND GET JUST ONE DEFINITE ANSWER PASSAGE CANDIDATE\n",
    "        indexPassage=0\n",
    "        for indexPassage in range(len(list_of_passages)):\n",
    "            NER_passage=get_Name_Entity_Sentence(list_of_passages[indexPassage])\n",
    "            for entity in NER_passage.items():\n",
    "                if (entity[1]==answer_type):\n",
    "                    candidate_passages[indexPassage]=get_number_of_common_kewyords(get_keyword(question),get_keyword(list_of_passages[indexPassage]))\n",
    "                    break\n",
    "\n",
    "\n",
    "        if len(candidate_passages)>0:\n",
    "            best_candidate_passage=list_of_passages[max(candidate_passages, key=candidate_passages.get)]\n",
    "        else:\n",
    "            if len(list_of_passages)>0:\n",
    "                best_candidate_passage=list_of_passages[0]\n",
    "        #print(\"Candidate Passage Answer:\")\n",
    "        #print(best_candidate_passage)\n",
    "\n",
    "       \n",
    "        NER_answer_passage=get_Name_Entity_Sentence(best_candidate_passage)\n",
    "        for entity in NER_answer_passage.items():\n",
    "                if (entity[1]==answer_type):\n",
    "                    answer=entity[0]\n",
    "    \n",
    "    #print('Predicted answer:',answer)\n",
    "    print(ida)\n",
    "    df_result.loc[len(df_result)]=[ida,answer]\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>june 16 , 1911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id          answer\n",
       "0  0  june 16 , 1911"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.to_csv('prediction/output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
