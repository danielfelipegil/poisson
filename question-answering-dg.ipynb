{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0. Exploratory analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "import pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "df_devel=pd.read_json('project_files/devel.json')\n",
    "df_docs=pd.read_json('project_files/documents.json')\n",
    "df_testing=pd.read_json('project_files/testing.json')\n",
    "\n",
    "df_training=pd.read_pickle('project_files/df_training.pkl')\n",
    "\n",
    "\n",
    "question_learning_dataset = df_training[df_training.answer_type.notnull()]\n",
    "\n",
    "NER_corpus=load_obj('ner_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer,PunktTrainer\n",
    "\n",
    "tokenizer = load_obj('punk_tokenizer')\n",
    "tokenizer._params.abbrev_types.add('ii')\n",
    "tokenizer._params.abbrev_types.add('dr')\n",
    "\n",
    "questionwords = set([\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"whose\", \"which\", \"whom\",\"whats\",\"what's\",\"whos\"])\n",
    "passiveQuestions = set([\"can\", \"could\", \"would\", \n",
    "                   \"was\", \"were\",\"am\",\"is\", \"are\", \"will\",\"shall\",\n",
    "                   \"did\",\"do\",\"does\",\n",
    "                   \"had\", \"have\",\"has\",\n",
    "                   \"as\",\"that\",\"in\",\n",
    "                   \"give an example\",\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find Keywords\n",
    "2. Answer types - Using answer type taxonomy\n",
    "3. Query formulation -> Keywords\n",
    "4. Go to each document and check the frequency distribution of words and pick the document if one of the query words are present in document. Create a rank with that score\n",
    "5. Find the paragraphs -> Discard irrelevant paragraphs. Use NE,Keywords, longest exact keywords. Put same weight for now and calculate the score of paragraphs. Rank each of the paragraphs in the document. We have to use the original answer and match the answer type\n",
    "6. Find candidate answers -> Use supervised ML method\n",
    "7. Merge candidate answers -> Use NER\n",
    "8. Pick the best answer -> Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Question processing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring Stanford CoreNLP . Link -> https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag.stanford import CoreNLPNERTagger\n",
    "from itertools import groupby\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "\n",
    "\n",
    "def get_Name_Entity_NLTK(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        ne_chunked_sents = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "        result = []\n",
    "\n",
    "        for tagged_tree in ne_chunked_sents:\n",
    "\n",
    "            if hasattr(tagged_tree, 'label'):\n",
    "                entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "                entity_type = tagged_tree.label() # get NE category\n",
    "                result.append((entity_name, entity_type))\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_Name_Entity_Sentence(sentence):\n",
    "    st = CoreNLPNERTagger(url='http://localhost:9000')\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    result = []\n",
    "    \n",
    "    for tag, chunk in groupby(classified_text, lambda x:x[1]):\n",
    "       if tag != \"O\":\n",
    "            word = \" \".join(w for w, t in chunk)\n",
    "            result.append((word.lower(), tag))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def addNameEntity(df,feature,func):\n",
    "    if 'NE'+\"_\"+feature in df:\n",
    "        df = df.drop('NE'+\"_\"+feature, axis=1)\n",
    "    df[\"NE\"+\"_\"+feature] = func(df[feature])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_type(question):\n",
    "    found  = False\n",
    "    result = 'other'\n",
    "    question_tokens = nltk.word_tokenize(question)\n",
    "    for token in question_tokens:\n",
    "        if token in questionwords:\n",
    "            found = True\n",
    "            result = token\n",
    "    if not found:\n",
    "        for token in question_tokens:\n",
    "            if token in passiveQuestions:\n",
    "                found = True\n",
    "                result = token\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "POS = set([\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"CD\",\"JJ\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]) \n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,wn.NOUN)\n",
    "    if (lemma == word):\n",
    "        lemma = lemmatizer.lemmatize(word,wn.VERB)\n",
    "        \n",
    "    return lemma\n",
    "\n",
    "def get_keyword(data):\n",
    "    result = []\n",
    "    sentence=data\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    for text,pos in tagged:\n",
    "        text = lemmatize(text.lower())\n",
    "        if text not in stopwords:\n",
    "            if pos in POS:\n",
    "                result.append(text)\n",
    "                \n",
    "    return result\n",
    "\n",
    "def get_keyword_paragraph(data):\n",
    "    results=[]\n",
    "    tokenized_sentence = tokenizer.tokenize(data)\n",
    "    for sentence in tokenized_sentence:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_keyword_all(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def add_keywords(df,feature):\n",
    "    if 'keywords'+\"_\"+feature in df:\n",
    "        df = df.drop('keywords'+\"_\"+feature, axis=1)\n",
    "    df['keywords'+\"_\"+feature]=get_keyword_all(df[feature])\n",
    "    return df\n",
    "\n",
    "def get_number_of_common_kewyords(question_keywords,answer_sentence_keywords):\n",
    "    sum_keywords=0\n",
    "    for qkey in question_keywords:\n",
    "        if qkey in answer_sentence_keywords:\n",
    "            sum_keywords+=1\n",
    "    \n",
    "    return sum_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train a classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW extraction for passages and questions\n",
    "def get_passages_bow(passages):\n",
    "    passage_bow={}\n",
    "    for passage in passages:\n",
    "        for token in nltk.word_tokenize(passage):\n",
    "            if token not in stopwords: \n",
    "                word=lemmatize(token.lower())\n",
    "                passage_bow[word] = passage_bow.get(word, 0) +  1\n",
    "    \n",
    "    return passage_bow\n",
    "\n",
    "def get_sentences_bow(sentences):\n",
    "    sentence_bow={}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for token in nltk.word_tokenize(sentence):\n",
    "            if token not in stopwords:\n",
    "                word=lemmatize(token.lower())\n",
    "                sentence_bow[word] = sentence_bow.get(word, 0) +  1\n",
    "    \n",
    "    return sentence_bow\n",
    "\n",
    "def get_question_bow(question):\n",
    "    question_bow={}\n",
    "    question_bow[get_question_type(question)]=1\n",
    "    for token in nltk.word_tokenize(question):\n",
    "        if token not in stopwords: \n",
    "            word=lemmatize(token.lower())\n",
    "            question_bow[word] = question_bow.get(word, 0) +  1\n",
    "                \n",
    "    return question_bow\n",
    "\n",
    "def get_training_question_bow(question,keywords,qt):\n",
    "    question_bow={}\n",
    "    question_bow[qt]=1\n",
    "    for token in nltk.word_tokenize(question):\n",
    "        if token not in stopwords: \n",
    "            word=lemmatize(token.lower())\n",
    "            if word in keywords:\n",
    "                question_bow[word] = question_bow.get(word, 0) +  1\n",
    "                \n",
    "    return question_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_questions(questions, keywords,qt):\n",
    "    qs = []\n",
    "    for i,question in enumerate(questions):\n",
    "        q_bow = get_training_question_bow(question,keywords,qt[i])\n",
    "        qs.append(q_bow)\n",
    "        \n",
    "    return qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def check_results(predictions, classifications):\n",
    "    print(\"Accuracy:\")\n",
    "    print(accuracy_score(classifications,predictions))\n",
    "    print(classification_report(classifications,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "    \n",
    "# get the most common words from answer sentences (we can twek this for paragraph)\n",
    "answer_sentences_bow=get_sentences_bow(question_learning_dataset[question_learning_dataset['answer_found'].notnull()]['answer_found'])\n",
    "answer_keywords = set([word for word, count in answer_sentences_bow.items()])\n",
    "\n",
    "#qs_training=get_feature_questions(questions,answer_keywords)\n",
    "qs_training=get_feature_questions(list(question_learning_dataset.question),answer_keywords,list(question_learning_dataset.question_type))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.8369626130814186\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "   CAUSE_OF_DEATH       1.00      0.47      0.64       327\n",
      "             CITY       1.00      0.08      0.15        12\n",
      "          COUNTRY       0.94      0.54      0.69      1058\n",
      "  CRIMINAL_CHARGE       1.00      0.33      0.49        64\n",
      "             DATE       0.76      0.99      0.86      5801\n",
      "         DURATION       0.96      0.64      0.77       464\n",
      "         IDEOLOGY       1.00      0.56      0.71       232\n",
      "         LOCATION       0.79      0.91      0.85      1738\n",
      "             MISC       1.00      0.50      0.67       133\n",
      "            MONEY       1.00      0.83      0.91       462\n",
      "      NATIONALITY       0.99      0.44      0.61       858\n",
      "           NUMBER       0.93      0.91      0.92      4644\n",
      "          ORDINAL       1.00      0.65      0.79       406\n",
      "     ORGANIZATION       1.00      0.51      0.68       496\n",
      "          PERCENT       0.98      0.85      0.91       751\n",
      "           PERSON       0.79      0.98      0.87      5468\n",
      "         RELIGION       0.99      0.62      0.76       493\n",
      "              SET       1.00      0.39      0.56       174\n",
      "STATE_OR_PROVINCE       1.00      0.43      0.60       335\n",
      "             TIME       1.00      0.42      0.59        53\n",
      "            TITLE       1.00      0.34      0.50      1000\n",
      "              URL       1.00      0.54      0.70        13\n",
      "\n",
      "      avg / total       0.87      0.84      0.82     24982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if (len(qs_training)>0 and len(list(question_learning_dataset.question_type))>0):\n",
    "    # fit vectorizer\n",
    "    vectorizer = DictVectorizer()\n",
    "    \n",
    "    X_train_dtm = vectorizer.fit_transform(qs_training)\n",
    "    \n",
    "    \n",
    "\n",
    "    model=RandomForestClassifier(n_estimators = 300, max_depth = 60, criterion = 'entropy')\n",
    "    \n",
    "    # tag the answers\n",
    "    # fit a logistic regression model to the data \n",
    "    # build classifier\n",
    "    #model = MultinomialNB(2, False, None)\n",
    "\n",
    "    # train the model using X_train_dtm \n",
    "    model.fit(X_train_dtm, list(question_learning_dataset.answer_type))\n",
    "    \n",
    "    y_predicted_class = model.predict(X_train_dtm)\n",
    "    \n",
    "    check_results(y_predicted_class,list(question_learning_dataset.answer_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Candidate answering generation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Get a score for the passage to filter the most relevant passages</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## features relevant to this part\n",
    "# number of named entities of the right type in the passage\n",
    "# number of question keywords in the passage\n",
    "# the longest exact sequence of question keywords\n",
    "# rank of the document where the passage was extracted\n",
    "# proximity of the keywords from the original query\n",
    "# ngram overlap between the passage and the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will set up useful functions to extract term frequencies to build the vector space model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) # wrap in a set() (see below)\n",
    "\n",
    "\n",
    "# get the terms for a passage\n",
    "def get_terms(passage):\n",
    "    terms = set()\n",
    "    for token in nltk.word_tokenize(passage):\n",
    "        if token not in stopwords: \n",
    "            terms.add(lemmatize(token.lower()))\n",
    "    return terms\n",
    "    \n",
    "# get document_term \n",
    "def get_document_term_passsages(ds_documents):\n",
    "    document_term={}\n",
    "    passageID=0\n",
    "    for index, row in ds_documents.iterrows():\n",
    "        passageID=0\n",
    "        terms={}\n",
    "        # every row is a document\n",
    "        list_of_passages=row['text']\n",
    "        for passage in list_of_passages:\n",
    "            terms[passageID]=get_terms(passage)\n",
    "            passageID+=1\n",
    "            \n",
    "        document_term[row['docid']]=terms\n",
    "    return document_term\n",
    "\n",
    "# get the term frequency\n",
    "def extract_term_freqs(doc):\n",
    "    tfs = Counter()\n",
    "    for token in doc:\n",
    "        if token not in stopwords: \n",
    "            tfs[lemmatize(token.lower())] += 1\n",
    "    return tfs\n",
    "        \n",
    "# compute idf\n",
    "def compute_doc_freqs(doc_term_freqs):\n",
    "    doc_dic = {}\n",
    "    for key, value in doc_term_freqs.items():\n",
    "        dfs = Counter()\n",
    "        for passage_id,tfs in value.items():\n",
    "            for term in tfs.keys():\n",
    "                dfs[term] += 1\n",
    "        doc_dic[key] = dfs\n",
    "        \n",
    "    return doc_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document-term matrix\n",
    "docs=get_document_term_passsages(df_docs)\n",
    "#docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector space model we need to define a score function\n",
    "# first I will use tf-idf\n",
    "doc_term_freqs = {}\n",
    "for docid,dic_passages in docs.items():\n",
    "    passage_dic = {}\n",
    "    for passage_id, terms in dic_passages.items():\n",
    "        term_freqs = extract_term_freqs(terms)\n",
    "        passage_dic[passage_id] = term_freqs\n",
    "    doc_term_freqs[docid] = passage_dic\n",
    "\n",
    "doc_freqs = compute_doc_freqs(doc_term_freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_term_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Improvement:</b> Use BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inverted index for query processing. Inverted index will not change from query to query. Here we can improve how the weight is defined for the posting list tuple for each term (docid,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(freqs):\n",
    "    p_count=0\n",
    "    for counter in freqs.values():\n",
    "        p_count+=sum(counter.values())\n",
    "    \n",
    "    #print(p_count)\n",
    "    return p_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code from WSTA_N16_information_retrieval\n",
    "vsm_inverted_index_all = defaultdict()\n",
    "for docid, passage_freqs in doc_term_freqs.items():\n",
    "    vsm_inverted_index = defaultdict(list)\n",
    "    \n",
    "    #N = sum(passage_freqs.values())\n",
    "    N = count_words(passage_freqs)\n",
    "    #print(N,passage_freqs)\n",
    "    for passage_id, term_freqs in passage_freqs.items():\n",
    "        length = 0\n",
    "        # find tf*idf values and accumulate sum of squares \n",
    "        tfidf_values = []\n",
    "        M = len(passage_freqs)\n",
    "        for term, count in term_freqs.items():\n",
    "            tfidf = float(count) / N * log(M / float(doc_freqs[docid][term])) # should be number of documents (paragraphs) with term\n",
    "            tfidf_values.append((term, tfidf))\n",
    "            length += tfidf ** 2\n",
    "\n",
    "        # normalise documents by length and insert into index\n",
    "        length = length ** 0.5\n",
    "        for term, tfidf in tfidf_values:\n",
    "            # note the inversion of the indexing, to be term -> (doc_id, score)\n",
    "            vsm_inverted_index[term].append([passage_id, tfidf / length])\n",
    "    vsm_inverted_index_all[docid] = vsm_inverted_index\n",
    "\n",
    "# ensure posting lists are in sorted order (less important here cf above)\n",
    "for key, value in vsm_inverted_index_all.items():\n",
    "    for term, docids in value.items():\n",
    "        docids.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsm_inverted_index_all=load_obj('vsm_inverted_index_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the VSM creating a score for each document (passage) and returning the top k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# get a list of paragraphs ordered by relevance on the question\n",
    "def query_vsm(query, index):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator\n",
    "\n",
    "##Â end copied code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Candidate answering scoring</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def correct_answer_space(predicted,predicted_answer_sentence):\n",
    "    \n",
    "    tokens=nltk.word_tokenize(predicted)\n",
    "    pattern='.*('\n",
    "    for token in  tokens:\n",
    "        pattern=pattern+token+'\\s*'\n",
    "    pattern=pattern+').*'\n",
    "\n",
    "    reg=re.compile(pattern,re.IGNORECASE)\n",
    "    if len(re.findall(reg,predicted_answer_sentence)):\n",
    "        predicted=re.findall(reg,predicted_answer_sentence)[0].strip()\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def correct_answer_pattern(predicted):\n",
    "    corrected=predicted\n",
    "    \n",
    "    # symbol % based pattern\n",
    "    pattern_percentaje='(.*[0-9])(%.*)'\n",
    "    reg=re.compile(pattern_percentaje,re.IGNORECASE)\n",
    "    result=re.findall(reg,predicted)\n",
    "    if len(result)>0:\n",
    "        groups=result[0]\n",
    "        corrected=groups[0]+' '+groups[1]\n",
    "    \n",
    "    \n",
    "    # date pattern\n",
    "    #pattern_date='(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\s+(\\d{1,2})(,.*)'\n",
    "    pattern_date='(January|February|March|April|May|June|July|August|September|October|November|December)\\s+([0-9]{1,2})(,.*)([0-9]{4})'\n",
    "\n",
    "\n",
    "    reg=re.compile(pattern_date,re.IGNORECASE)\n",
    "    result=re.findall(reg,predicted)\n",
    "\n",
    "    if len(result)>0:\n",
    "        groups=result[0]   \n",
    "        corrected=(groups[0]+' '+groups[1]+' '+groups[2]+groups[3])\n",
    "\n",
    "\n",
    "    return corrected\n",
    "    \n",
    "    \n",
    "def isAnswerInSentence(answer,answer_sentence):\n",
    "    #print('Eval:',answer)\n",
    "    #print('In:',answer_sentence)\n",
    "    tokens=nltk.word_tokenize(answer)\n",
    "    pattern='.*('\n",
    "    for token in  tokens:\n",
    "        pattern=pattern+token+'\\s*'\n",
    "    pattern=pattern+').*'\n",
    "\n",
    "    \n",
    "    reg=re.compile(pattern,re.IGNORECASE)\n",
    "    if len(re.findall(reg,answer_sentence))>0:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#df_devel=pd.read_json('project_files/devel.json')\n",
    "df_result_devel=pd.DataFrame(columns=['id','question','paragraph','retrieved paras','predicted_paragraph','paragraph_found','sentence','predicted_sentence','answer','predicted_answer'])\n",
    "df_devel=df_devel.iloc[0:100]\n",
    "\n",
    "for index, row in df_devel.iterrows():\n",
    "    t=time.process_time()\n",
    "    question=row['question']\n",
    "    docid=row['docid']\n",
    "    ida=index\n",
    "    \n",
    "    \n",
    "    question_keywords=get_keyword(question)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get the most relevant documents for the question\n",
    "    results = query_vsm(question_keywords, vsm_inverted_index_all[docid])\n",
    "    documents_ranked=results.most_common(10) \n",
    "    \n",
    "    \n",
    "    # prediction --not necessary if loading from pickle\n",
    "    q_bow=get_question_bow(question)\n",
    "    x = vectorizer.transform(q_bow)\n",
    "    answer_type=model.predict(x)\n",
    "    #answer_type=row['predicted_answer_type']\n",
    "    \n",
    "    passages_dict={}\n",
    "    sentences_dict={}\n",
    "    score_dict={}\n",
    "    answer=''\n",
    "    df_best_sentences=pd.DataFrame(columns=['doc_id','para_id','sentence_id','sentence_text','score'])\n",
    "    if len(documents_ranked)>0:\n",
    "        \n",
    "        for document in documents_ranked:\n",
    "            # perform a paragraph segmentation\n",
    "            paragraph_id=document[0]\n",
    "            paragraph_text=df_docs.iloc[docid]['text'][paragraph_id]\n",
    "            sentences_dict[paragraph_id]=tokenizer.tokenize(paragraph_text)\n",
    "        \n",
    "        score=0\n",
    "        for paragraph_dict in sentences_dict.items():\n",
    "            paragraph_id=paragraph_dict[0]\n",
    "            sentences_list=paragraph_dict[1]\n",
    "            \n",
    "            for sentence_index in range(len(sentences_list)):\n",
    "                NER_sentence=NER_corpus[docid][paragraph_id][sentence_index]\n",
    "                common_keywords=get_number_of_common_kewyords(question_keywords,get_keyword(sentences_list[sentence_index]))\n",
    "                similarity=nlp(question).similarity(nlp(sentences_list[sentence_index]))\n",
    "                score=common_keywords/len(question_keywords)+similarity\n",
    "                \n",
    "                for entity in NER_sentence:\n",
    "                    if (entity[1]==answer_type):\n",
    "                        score+=1\n",
    "                        break\n",
    "                        \n",
    "                df_best_sentences.loc[len(df_best_sentences)]=[docid,paragraph_id,sentence_index,sentences_list[sentence_index],score]\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        if len(df_best_sentences)>0: # answer='' otherwise\n",
    "                   \n",
    "            best_paragraph_id=df_best_sentences.loc[df_best_sentences.score.idxmax()]['para_id']\n",
    "            best_sentence_id=df_best_sentences.loc[df_best_sentences.score.idxmax()]['sentence_id']\n",
    "            best_sentence_text=df_best_sentences.loc[df_best_sentences.score.idxmax()]['sentence_text']\n",
    "            NER_answer_passage=NER_corpus[docid][best_paragraph_id][best_sentence_id]\n",
    "            for entity in NER_answer_passage:\n",
    "                if (entity[1]==answer_type):\n",
    "                    #print('answer:',entity[0])\n",
    "                    answer=correct_answer_pattern(entity[0])\n",
    "                    #print('corrected:',answer)\n",
    "                    break\n",
    "                    \n",
    "        \n",
    "    ## only for testing purposes - get the answer sentence and check the retrieved paragraph agains the selected\n",
    "    possible_par=[par[0] for par in documents_ranked]\n",
    "    par_retrieved=False\n",
    "    if row['answer_paragraph'] in possible_par:\n",
    "        par_retrieved=True\n",
    "        \n",
    "    sent_ans=''\n",
    "    \n",
    "    \n",
    "    for para in df_docs.iloc[docid]['text']:\n",
    "        sent_doc=tokenizer.tokenize(para)\n",
    "        #print(sent_doc)\n",
    "        for sent in sent_doc:\n",
    "            \n",
    "            if isAnswerInSentence(row['text'],sent):       \n",
    "                sent_ans=sent_doc\n",
    "                break\n",
    "    ## END -only for testing purposes\n",
    "    \n",
    "    print(ida,time.process_time()-t)\n",
    "    df_result_devel.loc[len(df_result_devel)]=[docid,question,row['answer_paragraph'],possible_par,best_paragraph_id,par_retrieved,sent_ans,best_sentence_text,row['text'],answer]\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - paragraph found: 99.0 %\n",
      "Accuracy - sentence predicted 57.99999999999999 %\n",
      "Accuracy - answer found: 17.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy - paragraph found:',df_result_devel.loc[df_result_devel['paragraph_found']==True]['id'].count()/len(df_result_devel)*100,'%')\n",
    "print('Accuracy - sentence predicted',len(df_result_devel.loc[df_result_devel.paragraph==df_result_devel.predicted_paragraph])/len(df_result_devel)*100,'%')\n",
    "print('Accuracy - answer found:',len(df_result_devel.loc[df_result_devel.predicted_answer==df_result_devel.answer])/len(df_result_devel)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_devel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result=pd.DataFrame(columns=['id','answer'])\n",
    "df_testing=pd.read_json('project_files/testing.json')\n",
    "#df_testing=df_testing.iloc[0:2]\n",
    "NER_dict={}\n",
    "for index, row in df_testing.iterrows():\n",
    "    t=time.process_time()\n",
    "    question=row['question']\n",
    "    docid=row['docid']\n",
    "    ida=row['id']\n",
    "    question_keywords=get_keyword(question)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get the most relevant documents for the question\n",
    "    results = query_vsm(question_keywords, vsm_inverted_index_all[docid])\n",
    "    documents_ranked=results.most_common(10) \n",
    "    \n",
    "    \n",
    "    # prediction --not necessary if loading from pickle\n",
    "    q_bow=get_question_bow(question)\n",
    "    x = vectorizer.transform(q_bow)\n",
    "    answer_type=model.predict(x)\n",
    "    \n",
    "    passages_dict={}\n",
    "    sentences_dict={}\n",
    "    score_dict={}\n",
    "    answer=''\n",
    "    df_best_sentences=pd.DataFrame(columns=['doc_id','para_id','sentence_id','sentence_text','score'])\n",
    "    if len(documents_ranked)>0:\n",
    "        \n",
    "        for document in documents_ranked:\n",
    "            # perform a paragraph segmentation\n",
    "            paragraph_id=document[0]\n",
    "            paragraph_text=df_docs.iloc[docid]['text'][paragraph_id]\n",
    "            sentences_dict[paragraph_id]=tokenizer.tokenize(paragraph_text)\n",
    "        \n",
    "        score=0\n",
    "        for paragraph_dict in sentences_dict.items():\n",
    "            paragraph_id=paragraph_dict[0]\n",
    "            sentences_list=paragraph_dict[1]\n",
    "            \n",
    "            for sentence_index in range(len(sentences_list)):\n",
    "                NER_sentence=NER_corpus[docid][paragraph_id][sentence_index]\n",
    "                common_keywords=get_number_of_common_kewyords(question_keywords,get_keyword(sentences_list[sentence_index]))\n",
    "                similarity=nlp(question).similarity(nlp(sentences_list[sentence_index]))\n",
    "                score=common_keywords/len(question_keywords)+similarity\n",
    "            \n",
    "                for entity in NER_sentence:\n",
    "                    if (entity[1]==answer_type):\n",
    "                        score+=1\n",
    "                        break\n",
    "                        \n",
    "                df_best_sentences.loc[len(df_best_sentences)]=[docid,paragraph_id,sentence_index,sentences_list[sentence_index],score]\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        if len(df_best_sentences)>0: # answer='' otherwise\n",
    "                   \n",
    "            best_paragraph_id=df_best_sentences.loc[df_best_sentences.score.idxmax()]['para_id']\n",
    "            best_sentence_id=df_best_sentences.loc[df_best_sentences.score.idxmax()]['sentence_id']\n",
    "            best_sentence_text=df_best_sentences.loc[df_best_sentences.score.idxmax()]['sentence_text']\n",
    "            NER_answer_passage=NER_corpus[docid][best_paragraph_id][best_sentence_id]\n",
    "            for entity in NER_answer_passage:\n",
    "                if (entity[1]==answer_type):\n",
    "                    answer=correct_answer_pattern(entity[0])\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "                    \n",
    "        \n",
    "            \n",
    "    if (answer==''):\n",
    "        \n",
    "        for entity in NER_answer_passage:\n",
    "            answer=correct_answer(entity[0],best_sentence_text)\n",
    "            break\n",
    "                    \n",
    "    print(ida,time.process_time()-t)\n",
    "    df_result.loc[len(df_result)]=[ida,answer]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('prediction/output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_result=df_result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import svm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "df_training=df_training[0:200]\n",
    "\n",
    "    \n",
    "def get_answer_rank_features_train():\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    t=time.process_time()\n",
    "    for index, row in df_training.iterrows():\n",
    "        question=row['question']\n",
    "        raw_answer=row['text']\n",
    "        answer_sentence=row['answer_found']\n",
    "        paragraph=df_docs.iloc[row['docid']]['text'][row['answer_paragraph']]\n",
    "        common_entities=0\n",
    "        \n",
    "        if (answer_sentence != None):\n",
    "            \n",
    "            #nlp\n",
    "            question_nlp=nlp(question)\n",
    "            answer_sentence_nlp=nlp(answer_sentence)\n",
    "\n",
    "            for entity in answer_sentence_nlp.sents:\n",
    "                if entity in question_nlp.ents:\n",
    "                    common_entities+=1\n",
    "\n",
    "\n",
    "            # number of question keywords in the passage\n",
    "            question_keywords=get_keyword(question)\n",
    "            answer_sentence_keywords=get_keyword(answer_sentence)\n",
    "            common_keywords=0\n",
    "            for qk in question_keywords:\n",
    "                if qk in answer_sentence_keywords:\n",
    "                    common_keywords+=1\n",
    "\n",
    "\n",
    "            # longest exact sequence of keywords\n",
    "            longest_exact_sequence=0\n",
    "            for i in range(len(question_keywords)):\n",
    "                if i < len(answer_sentence_keywords):\n",
    "                    if question_keywords[i] in answer_sentence_keywords[i]:\n",
    "                        longest_exact_sequence+=1\n",
    "\n",
    "\n",
    "            # similarity \n",
    "            similarity=question_nlp.similarity(answer_sentence_nlp)\n",
    "\n",
    "            # rank of the paragraph where the answer sentence was extracted\n",
    "            results = query_vsm(question_keywords, vsm_inverted_index_all[row['docid']])\n",
    "            documents_ranked=results.most_common(10) \n",
    "            rank_of_paragraph=0\n",
    "            for document in documents_ranked:\n",
    "                if (document[0]==row['answer_paragraph']):\n",
    "                    break\n",
    "                else:\n",
    "                    rank_of_paragraph+=1\n",
    "\n",
    "            # proximity\n",
    "            proximity=0\n",
    "            question_keywords_span=question_keywords.copy()\n",
    "            index_qk=0\n",
    "            while len(question_keywords_span)>0 and index_qk<len(question_keywords_span):\n",
    "                proximity+=1\n",
    "                if question_keywords_span[index_qk] in answer_sentence_keywords:\n",
    "                    question_keywords_span.pop(index_qk)\n",
    "                index_qk+=1\n",
    "\n",
    "            # n-gram overlap\n",
    "            bigrams_question =  nltk.bigrams([lemmatize(token) for token in nltk.word_tokenize(question)])\n",
    "            ngram_overlap=0\n",
    "\n",
    "            for bigram_question in bigrams_question:\n",
    "                bigrams_sentence = nltk.bigrams([lemmatize(token) for token in nltk.word_tokenize(answer_sentence)])\n",
    "                for bigram_sentence in bigrams_sentence:\n",
    "                    if bigram_question == bigram_sentence:\n",
    "                        ngram_overlap+=1\n",
    "\n",
    "\n",
    "\n",
    "            #print (num_entities,num_qkp,longest_exact_sequence,rank_of_paragraph)\n",
    "\n",
    "            tokenized_sentence = tokenizer.tokenize(paragraph)\n",
    "            for sentence in tokenized_sentence:\n",
    "                X.append([common_entities,common_keywords,longest_exact_sequence,rank_of_paragraph,proximity,ngram_overlap,similarity])\n",
    "                #print(sentence)\n",
    "                if (sentence==answer_sentence):\n",
    "                    Y.append(1)\n",
    "                else:\n",
    "                    Y.append(0)\n",
    "\n",
    "        print(index,time.process_time()-t)\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "X_train,y_train=get_answer_rank_features_train()\n",
    "\n",
    "# fit the model and get the separating hyperplane using weighted classes\n",
    "wclf = svm.SVC(kernel='linear', class_weight={1:4})\n",
    "wclf.fit(X, y)\n",
    "\n",
    "y_predicted_class = wclf.predict(X)\n",
    "\n",
    "\n",
    "classifications=y\n",
    "predictions=y_predicted_class\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy_score(classifications,predictions))\n",
    "print(classification_report(classifications,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "t=time.process_time()\n",
    "df_devel=df_devel[0:50]\n",
    "for index, row in df_devel.iterrows():\n",
    "    question=row['question']\n",
    "    raw_answer=row['text']\n",
    "    answer_sentence=row['answer_found']\n",
    "    paragraph=df_docs.iloc[row['docid']]['text'][row['answer_paragraph']]\n",
    "    common_entities=0\n",
    "\n",
    "    if (answer_sentence != None):\n",
    "\n",
    "        #nlp\n",
    "        question_nlp=nlp(question)\n",
    "        answer_sentence_nlp=nlp(answer_sentence)\n",
    "\n",
    "        for entity in answer_sentence_nlp.sents:\n",
    "            if entity in question_nlp.ents:\n",
    "                common_entities+=1\n",
    "\n",
    "\n",
    "        # number of question keywords in the passage\n",
    "        question_keywords=get_keyword(question)\n",
    "        answer_sentence_keywords=get_keyword(answer_sentence)\n",
    "        common_keywords=0\n",
    "        for qk in question_keywords:\n",
    "            if qk in answer_sentence_keywords:\n",
    "                common_keywords+=1\n",
    "\n",
    "\n",
    "        # longest exact sequence of keywords\n",
    "        longest_exact_sequence=0\n",
    "        for i in range(len(question_keywords)):\n",
    "            if i < len(answer_sentence_keywords):\n",
    "                if question_keywords[i] in answer_sentence_keywords[i]:\n",
    "                    longest_exact_sequence+=1\n",
    "\n",
    "\n",
    "        # similarity \n",
    "        similarity=question_nlp.similarity(answer_sentence_nlp)\n",
    "\n",
    "        # rank of the paragraph where the answer sentence was extracted\n",
    "        results = query_vsm(question_keywords, vsm_inverted_index_all[row['docid']])\n",
    "        documents_ranked=results.most_common(10) \n",
    "        rank_of_paragraph=0\n",
    "        for document in documents_ranked:\n",
    "            if (document[0]==row['answer_paragraph']):\n",
    "                break\n",
    "            else:\n",
    "                rank_of_paragraph+=1\n",
    "\n",
    "        # proximity\n",
    "        proximity=0\n",
    "        question_keywords_span=question_keywords.copy()\n",
    "        index_qk=0\n",
    "        while len(question_keywords_span)>0 and index_qk<len(question_keywords_span):\n",
    "            proximity+=1\n",
    "            if question_keywords_span[index_qk] in answer_sentence_keywords:\n",
    "                question_keywords_span.pop(index_qk)\n",
    "            index_qk+=1\n",
    "\n",
    "        # n-gram overlap\n",
    "        bigrams_question =  nltk.bigrams([lemmatize(token) for token in nltk.word_tokenize(question)])\n",
    "        ngram_overlap=0\n",
    "\n",
    "        for bigram_question in bigrams_question:\n",
    "            bigrams_sentence = nltk.bigrams([lemmatize(token) for token in nltk.word_tokenize(answer_sentence)])\n",
    "            for bigram_sentence in bigrams_sentence:\n",
    "                if bigram_question == bigram_sentence:\n",
    "                    ngram_overlap+=1\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        tokenized_sentence = tokenizer.tokenize(paragraph)\n",
    "        for sentence in tokenized_sentence:\n",
    "            X.append([common_entities,common_keywords,longest_exact_sequence,rank_of_paragraph,proximity,ngram_overlap,similarity])\n",
    "            #print(sentence)\n",
    "            if (sentence==answer_sentence):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(0)\n",
    "                \n",
    "                \n",
    "\n",
    "y_predicted_class = wclf.predict(X)\n",
    "\n",
    "\n",
    "classifications=y\n",
    "predictions=y_predicted_class\n",
    "\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy_score(classifications,predictions))\n",
    "print(classification_report(classifications,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
