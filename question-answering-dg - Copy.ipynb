{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0. Exploratory analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "import pickle\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_paragraph</th>\n",
       "      <th>docid</th>\n",
       "      <th>question</th>\n",
       "      <th>text</th>\n",
       "      <th>NE_question</th>\n",
       "      <th>NE_text</th>\n",
       "      <th>NE_paragraph</th>\n",
       "      <th>answer_type</th>\n",
       "      <th>keywords_question</th>\n",
       "      <th>question_type</th>\n",
       "      <th>POS_questions</th>\n",
       "      <th>answer_found</th>\n",
       "      <th>corrected_answer</th>\n",
       "      <th>NE_corrected_answer</th>\n",
       "      <th>NE_answer_found</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>A kilogram could be definined as having a Plan...</td>\n",
       "      <td>6966662606895999999♠6.62606896×10−34 j⋅s</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(6966662606895999999 ♠ 6.62606896, NUMBER), (...</td>\n",
       "      <td>[[(general, TITLE), (2011, DATE)], [], [(one, ...</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>[kilogram, definined, planck, constant, value]</td>\n",
       "      <td>what</td>\n",
       "      <td>[NN, VBN, NNP, NN, NN]</td>\n",
       "      <td>Possible new definitions include \"the mass of ...</td>\n",
       "      <td>6966662606895999999♠6.62606896×10−34 J⋅s</td>\n",
       "      <td>[(6966662606895999999 ♠ 6.62606896, NUMBER), (...</td>\n",
       "      <td>[(7050135639273999999 ♠ 135639274 ×, NUMBER), ...</td>\n",
       "      <td>NUMBER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the shape of the object that establish...</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[], [], [(1889, DATE), (paris, CITY)], [(1889...</td>\n",
       "      <td>None</td>\n",
       "      <td>[shape, object, establish, base, unit, kilogram]</td>\n",
       "      <td>what</td>\n",
       "      <td>[NN, NN, VBZ, JJ, NN, NN]</td>\n",
       "      <td>The most urgent unit on the list for redefinit...</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(1889, DATE), (paris, CITY)]</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>What example is given as another paired relati...</td>\n",
       "      <td>time vs. energy</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[], [], [(one, NUMBER)], [(fourier, LOCATION)]]</td>\n",
       "      <td>None</td>\n",
       "      <td>[example, give, pair, relationship, relate, st...</td>\n",
       "      <td>what</td>\n",
       "      <td>[NN, VBN, JJ, NN, VBN, JJ, NN]</td>\n",
       "      <td>One example is time vs. energy.</td>\n",
       "      <td>time vs. energy</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(one, NUMBER)]</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>What does the Planck Constant refer to?</td>\n",
       "      <td>quantum of action</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[], [(planck, PERSON)], [(now, DATE)], [], []]</td>\n",
       "      <td>None</td>\n",
       "      <td>[doe, planck, constant, refer]</td>\n",
       "      <td>what</td>\n",
       "      <td>[VBZ, NNP, NNP, NN]</td>\n",
       "      <td>Instead, it must be some multiple of a very sm...</td>\n",
       "      <td>quantum of action</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(now, DATE)]</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>When was the first quantized model of the atom...</td>\n",
       "      <td>1913</td>\n",
       "      <td>[(first, ORDINAL), (model, TITLE)]</td>\n",
       "      <td>[(1913, DATE)]</td>\n",
       "      <td>[[(niels bohr, PERSON), (first, ORDINAL), (mod...</td>\n",
       "      <td>DATE</td>\n",
       "      <td>[wa, first, quantize, model, atom, introduce]</td>\n",
       "      <td>when</td>\n",
       "      <td>[VBD, JJ, JJ, NN, NN, VBD]</td>\n",
       "      <td>Niels Bohr introduced the first quantized mode...</td>\n",
       "      <td>1913</td>\n",
       "      <td>[(1913, DATE)]</td>\n",
       "      <td>[(niels bohr, PERSON), (first, ORDINAL), (mode...</td>\n",
       "      <td>TIME</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_paragraph  docid                                           question  \\\n",
       "0                23      0  A kilogram could be definined as having a Plan...   \n",
       "1                22      0  What is the shape of the object that establish...   \n",
       "2                12      0  What example is given as another paired relati...   \n",
       "3                 1      0            What does the Planck Constant refer to?   \n",
       "4                10      0  When was the first quantized model of the atom...   \n",
       "\n",
       "                                       text  \\\n",
       "0  6966662606895999999♠6.62606896×10−34 j⋅s   \n",
       "1                                  cylinder   \n",
       "2                           time vs. energy   \n",
       "3                         quantum of action   \n",
       "4                                      1913   \n",
       "\n",
       "                          NE_question  \\\n",
       "0                                  []   \n",
       "1                                  []   \n",
       "2                                  []   \n",
       "3                                  []   \n",
       "4  [(first, ORDINAL), (model, TITLE)]   \n",
       "\n",
       "                                             NE_text  \\\n",
       "0  [(6966662606895999999 ♠ 6.62606896, NUMBER), (...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                     [(1913, DATE)]   \n",
       "\n",
       "                                        NE_paragraph answer_type  \\\n",
       "0  [[(general, TITLE), (2011, DATE)], [], [(one, ...      NUMBER   \n",
       "1  [[], [], [(1889, DATE), (paris, CITY)], [(1889...        None   \n",
       "2   [[], [], [(one, NUMBER)], [(fourier, LOCATION)]]        None   \n",
       "3    [[], [(planck, PERSON)], [(now, DATE)], [], []]        None   \n",
       "4  [[(niels bohr, PERSON), (first, ORDINAL), (mod...        DATE   \n",
       "\n",
       "                                   keywords_question question_type  \\\n",
       "0     [kilogram, definined, planck, constant, value]          what   \n",
       "1   [shape, object, establish, base, unit, kilogram]          what   \n",
       "2  [example, give, pair, relationship, relate, st...          what   \n",
       "3                     [doe, planck, constant, refer]          what   \n",
       "4      [wa, first, quantize, model, atom, introduce]          when   \n",
       "\n",
       "                    POS_questions  \\\n",
       "0          [NN, VBN, NNP, NN, NN]   \n",
       "1       [NN, NN, VBZ, JJ, NN, NN]   \n",
       "2  [NN, VBN, JJ, NN, VBN, JJ, NN]   \n",
       "3             [VBZ, NNP, NNP, NN]   \n",
       "4      [VBD, JJ, JJ, NN, NN, VBD]   \n",
       "\n",
       "                                        answer_found  \\\n",
       "0  Possible new definitions include \"the mass of ...   \n",
       "1  The most urgent unit on the list for redefinit...   \n",
       "2                    One example is time vs. energy.   \n",
       "3  Instead, it must be some multiple of a very sm...   \n",
       "4  Niels Bohr introduced the first quantized mode...   \n",
       "\n",
       "                           corrected_answer  \\\n",
       "0  6966662606895999999♠6.62606896×10−34 J⋅s   \n",
       "1                                  cylinder   \n",
       "2                           time vs. energy   \n",
       "3                         quantum of action   \n",
       "4                                      1913   \n",
       "\n",
       "                                 NE_corrected_answer  \\\n",
       "0  [(6966662606895999999 ♠ 6.62606896, NUMBER), (...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                     [(1913, DATE)]   \n",
       "\n",
       "                                     NE_answer_found   label  \n",
       "0  [(7050135639273999999 ♠ 135639274 ×, NUMBER), ...  NUMBER  \n",
       "1                      [(1889, DATE), (paris, CITY)]   OTHER  \n",
       "2                                    [(one, NUMBER)]   OTHER  \n",
       "3                                      [(now, DATE)]   OTHER  \n",
       "4  [(niels bohr, PERSON), (first, ORDINAL), (mode...    TIME  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_devel=pd.read_pickle('project_files/df_devel.pkl')\n",
    "df_docs=pd.read_json('project_files/documents.json')\n",
    "df_testing=pd.read_pickle('project_files/df_testing.pkl')\n",
    "df_training=pd.read_pickle('project_files/df_training.pkl')\n",
    "question_learning_dataset = df_training[df_training.answer_type.notnull()]\n",
    "question_devel_dataset = df_devel[df_devel.answer_type.notnull()]\n",
    "NER_corpus=load_obj('ner_corpus')\n",
    "model = load_obj('random_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29174 1969\n"
     ]
    }
   ],
   "source": [
    "print(len(question_learning_dataset),len(question_devel_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer,PunktTrainer\n",
    "\n",
    "tokenizer = load_obj('punk_tokenizer')\n",
    "tokenizer._params.abbrev_types.add('ii')\n",
    "tokenizer._params.abbrev_types.add('dr')\n",
    "\n",
    "questionwords = set([\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"whose\", \"which\", \"whom\",\"whats\",\"what's\",\"whos\"])\n",
    "passiveQuestions = set([\"can\", \"could\", \"would\", \n",
    "                   \"was\", \"were\",\"am\",\"is\", \"are\", \"will\",\"shall\",\n",
    "                   \"did\",\"do\",\"does\",\n",
    "                   \"had\", \"have\",\"has\",\n",
    "                   \"as\",\"that\",\"in\",\n",
    "                   \"give an example\",\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find Keywords\n",
    "2. Answer types - Using answer type taxonomy\n",
    "3. Query formulation -> Keywords\n",
    "4. Go to each document and check the frequency distribution of words and pick the document if one of the query words are present in document. Create a rank with that score\n",
    "5. Find the paragraphs -> Discard irrelevant paragraphs. Use NE,Keywords, longest exact keywords. Put same weight for now and calculate the score of paragraphs. Rank each of the paragraphs in the document. We have to use the original answer and match the answer type\n",
    "6. Find candidate answers -> Use supervised ML method\n",
    "7. Merge candidate answers -> Use NER\n",
    "8. Pick the best answer -> Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Question processing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring Stanford CoreNLP . Link -> https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tag.stanford import CoreNLPNERTagger\n",
    "from itertools import groupby\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "\n",
    "\n",
    "def get_Name_Entity_NLTK(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        ne_chunked_sents = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "        result = []\n",
    "\n",
    "        for tagged_tree in ne_chunked_sents:\n",
    "\n",
    "            if hasattr(tagged_tree, 'label'):\n",
    "                entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "                entity_type = tagged_tree.label() # get NE category\n",
    "                result.append((entity_name, entity_type))\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_Name_Entity_Sentence(sentence):\n",
    "    st = CoreNLPNERTagger(url='http://localhost:9000')\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    result = []\n",
    "    \n",
    "    for tag, chunk in groupby(classified_text, lambda x:x[1]):\n",
    "       if tag != \"O\":\n",
    "            word = \" \".join(w for w, t in chunk)\n",
    "            result.append((word.lower(), tag))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def addNameEntity(df,feature,func):\n",
    "    if 'NE'+\"_\"+feature in df:\n",
    "        df = df.drop('NE'+\"_\"+feature, axis=1)\n",
    "    df[\"NE\"+\"_\"+feature] = func(df[feature])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_type(question):\n",
    "    found  = False\n",
    "    result = 'other'\n",
    "    question_tokens = nltk.word_tokenize(question)\n",
    "    for token in question_tokens:\n",
    "        if token in questionwords:\n",
    "            found = True\n",
    "            result = token\n",
    "    if not found:\n",
    "        for token in question_tokens:\n",
    "            if token in passiveQuestions:\n",
    "                found = True\n",
    "                result = token\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "POS = set([\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"CD\",\"JJ\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]) \n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,wn.NOUN)\n",
    "    if (lemma == word):\n",
    "        lemma = lemmatizer.lemmatize(word,wn.VERB)\n",
    "        \n",
    "    return lemma\n",
    "\n",
    "def get_keyword(data):\n",
    "    result = []\n",
    "    sentence=data\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    for text,pos in tagged:\n",
    "        text = lemmatize(text.lower())\n",
    "        if text not in stopwords:\n",
    "            if pos in POS:\n",
    "                result.append(text)\n",
    "                \n",
    "    return result\n",
    "\n",
    "def get_keyword_paragraph(data):\n",
    "    results=[]\n",
    "    tokenized_sentence = tokenizer.tokenize(data)\n",
    "    for sentence in tokenized_sentence:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_keyword_all(data):\n",
    "    results=[]\n",
    "    for sentence in data:\n",
    "        result = get_keyword(sentence)\n",
    "        results.append(result)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def add_keywords(df,feature):\n",
    "    if 'keywords'+\"_\"+feature in df:\n",
    "        df = df.drop('keywords'+\"_\"+feature, axis=1)\n",
    "    df['keywords'+\"_\"+feature]=get_keyword_all(df[feature])\n",
    "    return df\n",
    "\n",
    "def get_number_of_common_kewyords(question_keywords,answer_sentence_keywords):\n",
    "    sum_keywords=0\n",
    "    for qkey in question_keywords:\n",
    "        if qkey in answer_sentence_keywords:\n",
    "            sum_keywords+=1\n",
    "    \n",
    "    return sum_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train a classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW extraction for passages and questions\n",
    "def get_passages_bow(passages):\n",
    "    passage_bow={}\n",
    "    for passage in passages:\n",
    "        for token in nltk.word_tokenize(passage):\n",
    "            if token not in stopwords: \n",
    "                word=lemmatize(token.lower())\n",
    "                passage_bow[word] = passage_bow.get(word, 0) +  1\n",
    "    \n",
    "    return passage_bow\n",
    "\n",
    "def get_sentences_bow(sentences):\n",
    "    sentence_bow={}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for token in nltk.word_tokenize(sentence):\n",
    "            if token not in stopwords:\n",
    "                word=lemmatize(token.lower())\n",
    "                sentence_bow[word] = sentence_bow.get(word, 0) +  1\n",
    "    \n",
    "    return sentence_bow\n",
    "\n",
    "def get_question_bow(question):\n",
    "    question_bow={}\n",
    "    question_bow[get_question_type(question)]=1\n",
    "    for token in nltk.word_tokenize(question):\n",
    "        if token not in stopwords: \n",
    "            word=lemmatize(token.lower())\n",
    "            question_bow[word] = question_bow.get(word, 0) +  1\n",
    "                \n",
    "    return question_bow\n",
    "\n",
    "def get_training_question_bow(question,keywords,qt):\n",
    "    question_bow={}\n",
    "    question_bow[qt]=1\n",
    "    for token in nltk.word_tokenize(question):\n",
    "        if token not in stopwords: \n",
    "            word=lemmatize(token.lower())\n",
    "            if word in keywords:\n",
    "                question_bow[word] = question_bow.get(word, 0) +  1\n",
    "                \n",
    "    return question_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_questions(questions, keywords,qt):\n",
    "    qs = []\n",
    "    for i,question in enumerate(questions):\n",
    "        q_bow = get_training_question_bow(question,keywords,qt[i])\n",
    "        qs.append(q_bow)\n",
    "        \n",
    "    return qs\n",
    "def get_feature_question(question, keywords,qt):\n",
    "#     print(question,keywords,qt)\n",
    "    q_bow = get_training_question_bow(question,keywords,qt)\n",
    "      \n",
    "    return q_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def check_results(predictions, classifications):\n",
    "    print(\"Accuracy:\")\n",
    "    print(accuracy_score(classifications,predictions))\n",
    "    print(classification_report(classifications,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "    \n",
    "# get the most common words from answer sentences (we can twek this for paragraph)\n",
    "answer_sentences_bow=get_sentences_bow(question_learning_dataset[question_learning_dataset['answer_found'].notnull()]['answer_found'])\n",
    "answer_keywords = set([word for word, count in answer_sentences_bow.items()])\n",
    "\n",
    "#qs_training=get_feature_questions(questions,answer_keywords)\n",
    "qs_training=get_feature_questions(list(question_learning_dataset.question),answer_keywords,list(question_learning_dataset.question_type))\n",
    "qs_dev=get_feature_questions(list(question_devel_dataset.question),answer_keywords,list(question_devel_dataset.question_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.7145759268664297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    LOCATION       0.70      0.53      0.60       226\n",
      "        MISC       0.00      0.00      0.00        51\n",
      "        NAME       0.79      0.25      0.38       184\n",
      "      NUMBER       0.90      0.78      0.83       466\n",
      "ORGANIZATION       0.75      0.08      0.15        72\n",
      "      PERSON       0.56      0.91      0.70       548\n",
      "        TIME       0.85      0.88      0.87       422\n",
      "\n",
      " avg / total       0.73      0.71      0.69      1969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if (len(qs_training)>0 and len(list(question_learning_dataset.question_type))>0):\n",
    "    # fit vectorizer\n",
    "    vectorizer = DictVectorizer()\n",
    "    \n",
    "    X_train_dtm = vectorizer.fit_transform(qs_training)\n",
    "    X_dev_dtm = vectorizer.transform(qs_dev)\n",
    "    \n",
    "\n",
    "    model=RandomForestClassifier(n_estimators = 300, max_depth = 60, criterion = 'entropy')\n",
    "    \n",
    "    # tag the answers\n",
    "    # fit a logistic regression model to the data \n",
    "    # build classifier\n",
    "    #model = MultinomialNB(2, False, None)\n",
    "\n",
    "    # train the model using X_train_dtm \n",
    "    model.fit(X_train_dtm, list(question_learning_dataset.label))\n",
    "    \n",
    "    y_predicted_class = model.predict(X_dev_dtm)\n",
    "    check_results(y_predicted_class,list(question_devel_dataset.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TIME'], dtype='<U12')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_dev_dtm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(model,'random_forest1')\n",
    "save_obj(vectorizer,'Vectorizer')\n",
    "save_obj(qs_training,'qs_training')\n",
    "save_obj(qs_dev,'qs_dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Candidate answering generation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Get a score for the passage to filter the most relevant passages</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will set up useful functions to extract term frequencies to build the vector space model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document-term matrix\n",
    "docs=get_document_term_passsages(df_docs)\n",
    "#docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector space model we need to define a score function\n",
    "# first I will use tf-idf\n",
    "doc_term_freqs = {}\n",
    "for docid,dic_passages in docs.items():\n",
    "    passage_dic = {}\n",
    "    for passage_id, terms in dic_passages.items():\n",
    "        term_freqs = extract_term_freqs(terms)\n",
    "        passage_dic[passage_id] = term_freqs\n",
    "    doc_term_freqs[docid] = passage_dic\n",
    "\n",
    "doc_freqs = compute_doc_freqs(doc_term_freqs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Improvement:</b> Use BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an inverted index for query processing. Inverted index will not change from query to query. Here we can improve how the weight is defined for the posting list tuple for each term (docid,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsm_inverted_index_all=load_obj('vsm_inverted_index_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the VSM creating a score for each document (passage) and returning the top k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# get a list of paragraphs ordered by relevance on the question\n",
    "def query_vsm(query, index):\n",
    "    accumulator = Counter()\n",
    "    for term in query:\n",
    "        postings = index[term]\n",
    "        for docid, weight in postings:\n",
    "            accumulator[docid] += weight\n",
    "    return accumulator\n",
    "\n",
    "## end copied code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Candidate answering scoring</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def correct_answer_space(predicted,predicted_answer_sentence):\n",
    "    \n",
    "    tokens=nltk.word_tokenize(predicted)\n",
    "    pattern='.*('\n",
    "    for token in  tokens:\n",
    "        pattern=pattern+token+'\\s*'\n",
    "    pattern=pattern+').*'\n",
    "\n",
    "    reg=re.compile(pattern,re.IGNORECASE)\n",
    "    if len(re.findall(reg,predicted_answer_sentence)):\n",
    "        predicted=re.findall(reg,predicted_answer_sentence)[0].strip()\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def correct_answer_pattern(predicted):\n",
    "    corrected=predicted\n",
    "    \n",
    "    # symbol % based pattern\n",
    "    pattern_percentaje='(.*[0-9])(%.*)'\n",
    "    reg=re.compile(pattern_percentaje,re.IGNORECASE)\n",
    "    result=re.findall(reg,predicted)\n",
    "    if len(result)>0:\n",
    "        groups=result[0]\n",
    "        corrected=groups[0]+' '+groups[1]\n",
    "    \n",
    "    \n",
    "    # date pattern\n",
    "    #pattern_date='(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)\\s+(\\d{1,2})(,.*)'\n",
    "    pattern_date='(January|February|March|April|May|June|July|August|September|October|November|December)\\s+([0-9]{1,2})(,.*)([0-9]{4})'\n",
    "\n",
    "\n",
    "    reg=re.compile(pattern_date,re.IGNORECASE)\n",
    "    result=re.findall(reg,predicted)\n",
    "\n",
    "    if len(result)>0:\n",
    "        groups=result[0]   \n",
    "        corrected=(groups[0]+' '+groups[1]+' '+groups[2]+groups[3])\n",
    "\n",
    "\n",
    "    return corrected\n",
    "    \n",
    "    \n",
    "def isAnswerInSentence(answer,answer_sentence):\n",
    "    #print('Eval:',answer)\n",
    "    #print('In:',answer_sentence)\n",
    "    tokens=nltk.word_tokenize(answer)\n",
    "    pattern='.*('\n",
    "    for token in  tokens:\n",
    "        pattern=pattern+token+'\\s*'\n",
    "    pattern=pattern+').*'\n",
    "\n",
    "    \n",
    "    reg=re.compile(pattern,re.IGNORECASE)\n",
    "    if len(re.findall(reg,answer_sentence))>0:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label={'NUMBER':'NUMBER','DATE':'TIME','PERSON':'PERSON','ORGANIZATION':'ORGANIZATION',\n",
    "      'MISC':'MISC','MONEY':'NUMBER','COUNTRY':'LOCATION',\n",
    "      'PERCENT':'NUMBER','TITLE':'PERSON','STATE_OR_PROVINCE':'LOCATION',\n",
    "      'CAUSE_OF_DEATH':'NAME','DURATION':'TIME','CRIMINAL_CHARGE':'NAME',\n",
    "       'CITY':'LOCATION','RELIGION':'NAME','SET':'TIME','NATIONALITY':'NAME',\n",
    "       'IDEOLOGY':'NAME','ORDINAL':'NUMBER','TIME':'TIME','URL':'ORGANIZATION',None:'OTHER','LOCATION':'LOCATION'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_paragraph</th>\n",
       "      <th>docid</th>\n",
       "      <th>question</th>\n",
       "      <th>text</th>\n",
       "      <th>question_type</th>\n",
       "      <th>NE_question</th>\n",
       "      <th>NE_text</th>\n",
       "      <th>keywords_question</th>\n",
       "      <th>answer_type</th>\n",
       "      <th>POS_questions</th>\n",
       "      <th>answer_found</th>\n",
       "      <th>corrected_answer</th>\n",
       "      <th>NE_corrected_answer</th>\n",
       "      <th>corrected_answer_type</th>\n",
       "      <th>label</th>\n",
       "      <th>NE_answer_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>380</td>\n",
       "      <td>On what date did the companies that became the...</td>\n",
       "      <td>june 16 , 1911</td>\n",
       "      <td>what</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(june 16 , 1911, DATE)]</td>\n",
       "      <td>[date, company, become, computing-tabulating-r...</td>\n",
       "      <td>DATE</td>\n",
       "      <td>[NN, NNS, VBD, NNP, NNP, VB, VBN]</td>\n",
       "      <td>On June 16, 1911, their four companies were co...</td>\n",
       "      <td>June 16, 1911</td>\n",
       "      <td>[(june 16 , 1911, DATE)]</td>\n",
       "      <td>DATE</td>\n",
       "      <td>TIME</td>\n",
       "      <td>[(june 16 , 1911, DATE), (four, NUMBER), (new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>380</td>\n",
       "      <td>What percentage of its desktop PCs does IBM pl...</td>\n",
       "      <td>5 %</td>\n",
       "      <td>what</td>\n",
       "      <td>[(ibm, ORGANIZATION)]</td>\n",
       "      <td>[(5 %, PERCENT)]</td>\n",
       "      <td>[percentage, desktop, pc, doe, ibm, plan, inst...</td>\n",
       "      <td>PERCENT</td>\n",
       "      <td>[NN, JJ, NN, VBZ, NNP, NN, VB, NNP, NNP]</td>\n",
       "      <td>IBM plans to install Open Client on 5% of its ...</td>\n",
       "      <td>5%</td>\n",
       "      <td>[(5 %, PERCENT)]</td>\n",
       "      <td>PERCENT</td>\n",
       "      <td>NUMBER</td>\n",
       "      <td>[(ibm, ORGANIZATION), (5 %, PERCENT)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>380</td>\n",
       "      <td>What year did IBM hire its first black salesman?</td>\n",
       "      <td>1946</td>\n",
       "      <td>what</td>\n",
       "      <td>[(year, DURATION), (ibm, ORGANIZATION), (first...</td>\n",
       "      <td>[(1946, DATE)]</td>\n",
       "      <td>[year, ibm, hire, first, black, salesman]</td>\n",
       "      <td>DATE</td>\n",
       "      <td>[NN, NNP, VB, JJ, JJ, NN]</td>\n",
       "      <td>In 1946, the company hired its first black sal...</td>\n",
       "      <td>1946</td>\n",
       "      <td>[(1946, DATE)]</td>\n",
       "      <td>DATE</td>\n",
       "      <td>TIME</td>\n",
       "      <td>[(1946, DATE), (first, ORDINAL), (salesman, TI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>380</td>\n",
       "      <td>IBM made an acquisition in 2009, name it.</td>\n",
       "      <td>spss</td>\n",
       "      <td>in</td>\n",
       "      <td>[(ibm, ORGANIZATION), (2009, DATE)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ibm, make, acquisition, 2009, name]</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>[NNP, VBD, NN, CD, NN]</td>\n",
       "      <td>IBM acquired Kenexa (2012) and SPSS (2009) and...</td>\n",
       "      <td>SPSS</td>\n",
       "      <td>[(spss, ORGANIZATION)]</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>[(ibm, ORGANIZATION), (kenexa, PERSON), (2012,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>380</td>\n",
       "      <td>This IBM invention is known by the acronym UPC...</td>\n",
       "      <td>universal product code</td>\n",
       "      <td>what</td>\n",
       "      <td>[(ibm, ORGANIZATION), (upc, ORGANIZATION)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ibm, invention, know, acronym, upc, full, name]</td>\n",
       "      <td>None</td>\n",
       "      <td>[NNP, NN, VBN, NN, NNP, JJ, NN]</td>\n",
       "      <td>Notable company inventions or developments inc...</td>\n",
       "      <td>Universal Product Code</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>[(teller, TITLE), (upc, ORGANIZATION), (fortra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_paragraph  docid                                           question  \\\n",
       "0                 5    380  On what date did the companies that became the...   \n",
       "1                22    380  What percentage of its desktop PCs does IBM pl...   \n",
       "2                16    380   What year did IBM hire its first black salesman?   \n",
       "3                 4    380          IBM made an acquisition in 2009, name it.   \n",
       "4                 2    380  This IBM invention is known by the acronym UPC...   \n",
       "\n",
       "                     text question_type  \\\n",
       "0          june 16 , 1911          what   \n",
       "1                     5 %          what   \n",
       "2                    1946          what   \n",
       "3                    spss            in   \n",
       "4  universal product code          what   \n",
       "\n",
       "                                         NE_question  \\\n",
       "0                                                 []   \n",
       "1                              [(ibm, ORGANIZATION)]   \n",
       "2  [(year, DURATION), (ibm, ORGANIZATION), (first...   \n",
       "3                [(ibm, ORGANIZATION), (2009, DATE)]   \n",
       "4         [(ibm, ORGANIZATION), (upc, ORGANIZATION)]   \n",
       "\n",
       "                    NE_text  \\\n",
       "0  [(june 16 , 1911, DATE)]   \n",
       "1          [(5 %, PERCENT)]   \n",
       "2            [(1946, DATE)]   \n",
       "3                        []   \n",
       "4                        []   \n",
       "\n",
       "                                   keywords_question   answer_type  \\\n",
       "0  [date, company, become, computing-tabulating-r...          DATE   \n",
       "1  [percentage, desktop, pc, doe, ibm, plan, inst...       PERCENT   \n",
       "2          [year, ibm, hire, first, black, salesman]          DATE   \n",
       "3               [ibm, make, acquisition, 2009, name]  ORGANIZATION   \n",
       "4   [ibm, invention, know, acronym, upc, full, name]          None   \n",
       "\n",
       "                              POS_questions  \\\n",
       "0         [NN, NNS, VBD, NNP, NNP, VB, VBN]   \n",
       "1  [NN, JJ, NN, VBZ, NNP, NN, VB, NNP, NNP]   \n",
       "2                 [NN, NNP, VB, JJ, JJ, NN]   \n",
       "3                    [NNP, VBD, NN, CD, NN]   \n",
       "4           [NNP, NN, VBN, NN, NNP, JJ, NN]   \n",
       "\n",
       "                                        answer_found        corrected_answer  \\\n",
       "0  On June 16, 1911, their four companies were co...           June 16, 1911   \n",
       "1  IBM plans to install Open Client on 5% of its ...                      5%   \n",
       "2  In 1946, the company hired its first black sal...                    1946   \n",
       "3  IBM acquired Kenexa (2012) and SPSS (2009) and...                    SPSS   \n",
       "4  Notable company inventions or developments inc...  Universal Product Code   \n",
       "\n",
       "        NE_corrected_answer corrected_answer_type         label  \\\n",
       "0  [(june 16 , 1911, DATE)]                  DATE          TIME   \n",
       "1          [(5 %, PERCENT)]               PERCENT        NUMBER   \n",
       "2            [(1946, DATE)]                  DATE          TIME   \n",
       "3    [(spss, ORGANIZATION)]          ORGANIZATION  ORGANIZATION   \n",
       "4                        []                  None         OTHER   \n",
       "\n",
       "                                     NE_answer_found  \n",
       "0  [(june 16 , 1911, DATE), (four, NUMBER), (new ...  \n",
       "1              [(ibm, ORGANIZATION), (5 %, PERCENT)]  \n",
       "2  [(1946, DATE), (first, ORDINAL), (salesman, TI...  \n",
       "3  [(ibm, ORGANIZATION), (kenexa, PERSON), (2012,...  \n",
       "4  [(teller, TITLE), (upc, ORGANIZATION), (fortra...  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_devel1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n"
     ]
    }
   ],
   "source": [
    "df_result_devel=pd.DataFrame(columns=['id','question','paragraph','retrieved paras','predicted_paragraph',\n",
    "                                      'paragraph_found','sentence','predicted_sentence','predicted_sentence_id',\n",
    "                                      'sentence_found','answer','predicted_answer','NE_question','label'])\n",
    "df_devel1=df_devel.iloc[0:100]\n",
    "count = 0\n",
    "for index, row in df_devel1.iterrows():\n",
    "    question=row['question']\n",
    "    docid=row['docid']\n",
    "    ida=index\n",
    "    question_keywords=get_keyword(question)\n",
    "    results = query_vsm(question_keywords, vsm_inverted_index_all[docid])\n",
    "    documents_ranked=results.most_common(10) \n",
    "    best_dic = set()\n",
    "    cc = 0\n",
    "    found = False\n",
    "    possible_par=[par[0] for par in documents_ranked]\n",
    "    best_paragraph_id = -1\n",
    "    par_retrieved = False\n",
    "    for doc in documents_ranked:\n",
    "        if(cc>1):\n",
    "            break\n",
    "        cc = cc + 1\n",
    "        best_dic.add(doc[0])\n",
    "    if(row['answer_paragraph'] in best_dic):\n",
    "        count = count + 1\n",
    "        par_retrieved = True\n",
    "        best_paragraph_id = row['answer_paragraph']\n",
    "    df_result_devel.loc[ida] = [docid,question,row['answer_paragraph'],\n",
    "                                possible_par,best_paragraph_id,\n",
    "                                par_retrieved,row['answer_found'],None,None,False,row['text'],None,\n",
    "                                row['NE_question'],row['label']]\n",
    "\n",
    "print(count*1.0/len(df_devel1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "df_result_devel = df_result_devel[df_result_devel['paragraph_found']==True]\n",
    "print(len(df_result_devel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "df_best_sentences=pd.DataFrame(columns=['doc_id','para_id','original','sentence_id','sentence_text','score','sentence'])\n",
    "\n",
    "for index,row in df_result_devel.iterrows():\n",
    "    NE_question = row['NE_question']\n",
    "    paragraph = df_docs.iloc[row['id']]['text'][row['predicted_paragraph']]\n",
    "    paragraph_id = row['predicted_paragraph']\n",
    "    question = row['question']\n",
    "    answer = row['sentence']\n",
    "    docid = row['id']\n",
    "    candidate_answers = tokenizer.tokenize(paragraph)\n",
    "    question_keywords=get_keyword(question)\n",
    "    sentences_check = Counter()\n",
    "    for sentence_index,sentence in enumerate(candidate_answers):\n",
    "        NER_sentence=NER_corpus[docid][paragraph_id][sentence_index]\n",
    "        answer_sentence_keywords = get_keyword(sentence)\n",
    "        common_keywords=get_number_of_common_kewyords(question_keywords,answer_sentence_keywords)\n",
    "        common_entities = get_number_of_common_entities(NER_sentence,NE_question)\n",
    "        \n",
    "        # longest exact sequence of keywords\n",
    "        longest_exact_sequence=0\n",
    "        for i in range(len(question_keywords)):\n",
    "            if i < len(answer_sentence_keywords):\n",
    "                if question_keywords[i] in answer_sentence_keywords[i]:\n",
    "                    longest_exact_sequence+=1\n",
    "#                     print(question_keywords[i],answer_sentence_keywords[i])\n",
    "#         print(\"Longest->\",longest_exact_sequence)\n",
    "        \n",
    "        # proximity\n",
    "        proximity=0\n",
    "        question_keywords_span=question_keywords.copy()\n",
    "        index_qk=0\n",
    "        while len(question_keywords_span)>0 and index_qk<len(question_keywords_span):\n",
    "            proximity+=1\n",
    "            if question_keywords_span[index_qk] in answer_sentence_keywords:\n",
    "                question_keywords_span.pop(index_qk)\n",
    "            index_qk+=1\n",
    "#         print(\"proximity->\",proximity)\n",
    "        \n",
    "        # n-gram overlap\n",
    "        bigrams_question =  nltk.bigrams([lemmatize(token) for token in nltk.word_tokenize(question)])\n",
    "        ngram_overlap=0\n",
    "\n",
    "        for bigram_question in bigrams_question:\n",
    "            bigrams_sentence = nltk.bigrams([lemmatize(token) for token in nltk.word_tokenize(sentence)])\n",
    "            for bigram_sentence in bigrams_sentence:\n",
    "                if bigram_question == bigram_sentence:\n",
    "#                     print(bigram_question,'sentence',bigram_sentence)\n",
    "                    ngram_overlap+=1\n",
    "#         print(\"n-gram->\",ngram_overlap)\n",
    "        sentence_score = common_keywords + common_entities + longest_exact_sequence + proximity + ngram_overlap\n",
    "        df_best_sentences.loc[len(df_best_sentences)]=[row['id'],\n",
    "                                                       paragraph_id,row['paragraph'],sentence_index,\n",
    "                                                       sentence,sentence_score,row['sentence']]\n",
    "        sentences_check[sentence_index] = sentence_score\n",
    "    \n",
    "    best = sentences_check.most_common()\n",
    "    print(index)\n",
    "    df_result_devel.at[index, 'predicted_sentence_id'] = best[0][0]\n",
    "    df_result_devel.at[index, 'predicted_sentence'] = candidate_answers[best[0][0]]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7741935483870968\n"
     ]
    }
   ],
   "source": [
    "count  = 0\n",
    "for index,row in df_result_devel.iterrows():\n",
    "    sentence = row['sentence']\n",
    "    predicted_sentence = row['predicted_sentence']\n",
    "    if(sentence==predicted_sentence):\n",
    "        count = count + 1\n",
    "        df_result_devel.at[index, 'sentence_found'] = True\n",
    "print(count*1.0/len(df_result_devel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_common_entities(ans,question):\n",
    "    question_NE = set(x[1] for x in question)\n",
    "    answer_NE = set(x[1] for x in ans)\n",
    "    sum_keywords=0\n",
    "    for qkey in question_NE:\n",
    "        if qkey in answer_NE:\n",
    "            sum_keywords+=1\n",
    "#     print(ans,question)\n",
    "    return sum_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "df_result_devel1 = df_result_devel[df_result_devel['sentence_found']==True]\n",
    "print(len(df_result_devel1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "df_result_devel1['predicted_label'] = None\n",
    "\n",
    "for index,row in df_result_devel1.iterrows():\n",
    "    sentence = row['predicted_sentence']\n",
    "    question = row['question']\n",
    "    question_NER = row['NE_question']\n",
    "    sentence_id = row['predicted_sentence_id']\n",
    "    paragraph_id = row['predicted_paragraph']\n",
    "    docid = row['id']\n",
    "    answer = row['answer']\n",
    "    sentence_NER = NER_corpus[docid][paragraph_id][sentence_id]\n",
    "    sentence_keywords = get_keyword(sentence)\n",
    "    question_keywords=get_keyword(question)\n",
    "    predicted_label = model.predict(vectorizer.transform(\n",
    "        get_feature_question(question,answer_keywords,get_question_type(question))))[0]\n",
    "    \n",
    "    df_result_devel1.at[index, 'predicted_label'] = predicted_label\n",
    "    \n",
    "    answer_type = None\n",
    "    predicted_answer = None\n",
    "    for NE in sentence_NER:\n",
    "        if (label[NE[1]]==predicted_label):\n",
    "            answer_type = NE[1]\n",
    "            predicted_answer = NE[0]\n",
    "            break\n",
    "#     print(index,answer,predicted_label,answer_type,predicted_answer)\n",
    "    if(answer_type):\n",
    "        df_result_devel1.at[index, 'predicted_answer'] = predicted_answer\n",
    "        count = count + 1\n",
    "#     print(index)\n",
    "print(count)\n",
    "#     else:\n",
    "#         print(sentence,\"\"predicted_sentence)\n",
    "#         print(index,answer,predicted_label,answer_type)\n",
    "#         break\n",
    "#     print(index,question,paragraph_id,sentence_id,\n",
    "#           sentence,sentence_NER,sentence_keywords,question_keywords,answer,predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_result_devel1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for index,row in df_result_devel1.iterrows():\n",
    "    ans = row['label']\n",
    "    pred = row['predicted_label']\n",
    "    if(ans == pred):\n",
    "        count  = count + 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER ~~ NUMBER ~~ second largest ~~ second\n",
      "NUMBER ~~ NUMBER ~~ 4,100 gallons ~~ six\n",
      "TIME ~~ TIME ~~ '07 ~~ november 2010\n",
      "PERSON ~~ PERSON ~~ blue gene ~~ gene\n",
      "PERSON ~~ PERSON ~~ albert l. williams ~~ thomas j. watson\n",
      "NUMBER ~~ NUMBER ~~ 34 ~~ first\n",
      "TIME ~~ TIME ~~ 2013 ~~ 2004\n",
      "LOCATION ~~ LOCATION ~~ the u.s. south ~~ u.s.\n",
      "strange None charles ranlett flint ~~ None\n",
      "NUMBER ~~ NUMBER ~~ 78,000 gallons ~~ 78,000\n",
      "NUMBER ~~ NUMBER ~~ five nobel prizes ~~ five\n",
      "NUMBER ~~ NUMBER ~~ fourth largest ~~ second\n",
      "NUMBER ~~ NUMBER ~~ 26 million ~~ first\n",
      "NUMBER ~~ NUMBER ~~ 5.5 ~~ 5.5 million\n",
      "NUMBER ~~ NUMBER ~~ 53 ~~ 400\n",
      "LOCATION ~~ LOCATION ~~ tennessee ~~ oak ridge\n",
      "16 8 7\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "count = 0\n",
    "count1 = 0\n",
    "count3 = 0\n",
    "for index,row in df_result_devel1.iterrows():\n",
    "    ans = row['answer']\n",
    "    pred = row['predicted_answer']\n",
    "    if(row['label']==row['predicted_label']):\n",
    "        if(ans == pred):\n",
    "            count = count + 1\n",
    "        else:\n",
    "            if(pred):\n",
    "                seq=difflib.SequenceMatcher(None, ans,pred)\n",
    "                d=seq.ratio()*100\n",
    "                if (d>20.00):\n",
    "    #                 print(index,row['question'])\n",
    "                    print(row['label'],\"~~\",row['predicted_label'],'~~',ans,\"~~\",pred)\n",
    "                    count1 = count1 + 1\n",
    "                else:\n",
    "    #                 print(index,row['question'])\n",
    "                    print(row['label'],\"~~\",row['predicted_label'],'~~',ans,\"~~\",pred)\n",
    "                    count3 = count3 + 1\n",
    "            else:\n",
    "                print(\"strange\",pred,row['answer'],\"~~\",row[\"predicted_answer\"])\n",
    "print(count,count1,count3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
